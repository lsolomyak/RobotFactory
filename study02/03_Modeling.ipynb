{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e15e19bc-01c4-491e-9103-c9a5aa19836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame, read_csv, concat\n",
    "from arviz import hdi\n",
    "sns.set_theme(style='ticks', context='notebook', font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27606e97-1714-476d-870b-626405633959",
   "metadata": {},
   "source": [
    "## Section 1: Model Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ea07c2f-51a1-4b3e-93b6-b81d7ea20f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define parameters.\n",
    "models = ['pgng_m1', 'pgng_m2', 'pgng_m3', 'pgng_m4', 'pgng_m5', 'pgng_m6', 'pgng_m7']\n",
    "sessions = ['s1', 's2', 's3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc2547-5f04-4b8e-a733-c5abae87c10a",
   "metadata": {},
   "source": [
    "### 1.1 Stan diagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57932b41-05ff-415e-b330-82d55f6c3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main loop.\n",
    "diagnostics = []\n",
    "for m in models:\n",
    "    \n",
    "    for s in sessions:\n",
    "\n",
    "        ## Load Stan summary.\n",
    "        samples = read_csv(os.path.join('stan_results', s, f'{m}.tsv.gz'), sep='\\t', compression='gzip')\n",
    "        summary = read_csv(os.path.join('stan_results', s, f'{m}_summary.tsv'), sep='\\t', index_col=0)\n",
    "        ppc = read_csv(os.path.join('stan_results', s, f'{m}_ppc.csv'))\n",
    "\n",
    "        ## Apply restrictions.\n",
    "        ppc = ppc[~np.isinf(ppc.k_u)]                        # Removed fixed parameters.\n",
    "\n",
    "        ## Identify number of divergences.\n",
    "        divergence = samples.divergent__.sum()\n",
    "\n",
    "        ## Identify parameters failing to reach convergence.\n",
    "        rhat = len(summary.query('R_hat >= 1.02'))\n",
    "\n",
    "        ## Identify parameters with low effective sample size.\n",
    "        n_eff = len(summary.query('N_Eff < 400'))\n",
    "\n",
    "        ## Identify number of effective parameters.\n",
    "        p_loo = ppc.pwaic.sum()\n",
    "\n",
    "        ## Identify number of poorly predicted observations.\n",
    "        pk = np.sum(ppc.k_u > 0.7)\n",
    "\n",
    "        ## Convert to dictionary. Append.\n",
    "        diagnostics.append(dict(\n",
    "            model = m,\n",
    "            session = s,\n",
    "            divergence = divergence,\n",
    "            rhat = rhat,\n",
    "            n_eff = n_eff,\n",
    "            p_loo = np.round(p_loo, 1),\n",
    "            pk = np.round(pk, 3)\n",
    "        ))\n",
    "\n",
    "## Convert to DataFrame.\n",
    "diagnostics = DataFrame(diagnostics).sort_values(['session','model']).set_index(['session','model'])\n",
    "diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d85235a-cd42-4671-9c8b-b62e5a2778f5",
   "metadata": {},
   "source": [
    "## Section 2: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45332ee2-aafd-4d4a-92df-e5121e2f7028",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define parameters.\n",
    "models = ['pgng_m1', 'pgng_m2', 'pgng_m3', 'pgng_m4', 'pgng_m5', 'pgng_m6', 'pgng_m7']\n",
    "sessions = ['s1', 's2', 's3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae518f-2c94-4b03-bef8-7028cb064bbc",
   "metadata": {},
   "source": [
    "### 2.1 LOO-CV indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca416c65-63ee-424b-b633-75d13895cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main loop.\n",
    "loocv = []\n",
    "for m in models:\n",
    "\n",
    "    for s in sessions:\n",
    "    \n",
    "        ## Load posterior predictive check.\n",
    "        ppc = read_csv(os.path.join('stan_results', s, f'{m}_ppc.csv'))\n",
    "\n",
    "        ## Compute LOO-CV.\n",
    "        loo = -2 * ppc.loo.sum()\n",
    "\n",
    "        ## Convert to dictionary. Append.\n",
    "        loocv.append(dict(model=m, session=s, loocv=loo))\n",
    "        \n",
    "## Convert to DataFrame.\n",
    "loocv = DataFrame(loocv).pivot_table('loocv', 'session', 'model')\n",
    "loocv.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff939637-96fa-4ca8-ac53-77fa68f56879",
   "metadata": {},
   "source": [
    "### 2.2 Model comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e86514-c6bf-4c8c-b7ff-94c969c7b5fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         se \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(arr) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(N)\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m## Convert to dictionary. Append.\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m         loocv\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mdict\u001b[39m(model\u001b[38;5;241m=\u001b[39m\u001b[43mm\u001b[49m, session\u001b[38;5;241m=\u001b[39ms, a\u001b[38;5;241m=\u001b[39ma, b\u001b[38;5;241m=\u001b[39mb, loocv\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%0.1f\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%0.1f\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m(mu, se)))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m## Convert to DataFrame.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loocv \u001b[38;5;241m=\u001b[39m DataFrame(loocv)\u001b[38;5;241m.\u001b[39mpivot_table(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloocv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msession\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m], aggfunc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "loocv = []\n",
    "for s in sessions:\n",
    "\n",
    "    for a, b in list(combinations(models, 2)):\n",
    "\n",
    "        ## Load data.\n",
    "        ppc1 = read_csv(os.path.join('stan_results', s, f'{a}_ppc.csv'))\n",
    "        ppc2 = read_csv(os.path.join('stan_results', s, f'{b}_ppc.csv'))\n",
    "\n",
    "        arr = -2 * (ppc2.loo - ppc1.loo)\n",
    "\n",
    "        ## Compute stats.\n",
    "        N = len(ppc1)\n",
    "        mu = np.sum(arr)\n",
    "        se = np.std(arr) * np.sqrt(N)\n",
    "\n",
    "        ## Convert to dictionary. Append.\n",
    "        loocv.append(dict(model=m, session=s, a=a, b=b, loocv='%0.1f (%0.1f)' %(mu, se)))\n",
    "        \n",
    "## Convert to DataFrame.\n",
    "loocv = DataFrame(loocv).pivot_table('loocv', 'a', ['session','b'], aggfunc=lambda x: x).fillna('-')\n",
    "loocv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8f404d-14ed-40ab-b6c0-5d85826e96a4",
   "metadata": {},
   "source": [
    "### 2.3 Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a63d7-3057-4e33-aee0-cc627bf4e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "## Define winning model.\n",
    "winning = 'pgng_m7'\n",
    "\n",
    "## Main loop.\n",
    "loocv = []\n",
    "for i, m in enumerate(models):\n",
    "\n",
    "    ## Load posterior predictive checks.\n",
    "    ppc1 = concat([read_csv(os.path.join('stan_results', s, f'{m}_ppc.csv'))\n",
    "                  for s in sessions])\n",
    "\n",
    "    ## Load posterior predictive checks.\n",
    "    ppc2 = concat([read_csv(os.path.join('stan_results', s, f'{winning}_ppc.csv'))\n",
    "                  for s in sessions])\n",
    "    \n",
    "    ## Compute classification accuracy.\n",
    "    score = accuracy_score(ppc1.choice, ppc1.Y_hat > 0.5) * 1e2\n",
    "    \n",
    "    ## Compute LOO-CV.\n",
    "    loo = -2 * ppc1.loo.sum()\n",
    "    \n",
    "    ## Comute delta LOO-CV.\n",
    "    arr = -2 * (ppc2.loo - ppc1.loo)\n",
    "    mu = np.sum(arr)\n",
    "    se = np.std(arr) * np.sqrt(len(arr))\n",
    "    \n",
    "    ## Append.\n",
    "    loocv.append(dict(model=i+1, score='%0.1f' %score, loo='%0.1f' %loo, delta='%0.1f (%0.1f)' %(mu, se)))\n",
    "    \n",
    "## Convert to DataFrame.\n",
    "loocv = DataFrame(loocv).set_index('model')\n",
    "loocv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a77c9cb-c7fb-49a2-aef6-83a4f40325cf",
   "metadata": {},
   "source": [
    "### 2.4 Table S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2110562-38e7-4a2a-a5fc-b61b5e461ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "## Define winning model.\n",
    "winning = 'pgng_m7'\n",
    "\n",
    "## Main loop.\n",
    "loocv = []\n",
    "for i, m in enumerate(models):\n",
    "\n",
    "    for s in sessions:\n",
    "    \n",
    "        ## Load posterior predictive checks.\n",
    "        ppc1 = read_csv(os.path.join('stan_results', s, f'{m}_ppc.csv'))\n",
    "\n",
    "        ## Load posterior predictive checks.\n",
    "        ppc2 = read_csv(os.path.join('stan_results', s, f'{winning}_ppc.csv'))\n",
    "\n",
    "        ## Compute classification accuracy.\n",
    "        score = accuracy_score(ppc1.choice, ppc1.Y_hat > 0.5) * 1e2\n",
    "\n",
    "        ## Compute LOO-CV.\n",
    "        loo = -2 * ppc1.loo.sum()\n",
    "\n",
    "        ## Comute delta LOO-CV.\n",
    "        arr = -2 * (ppc2.loo - ppc1.loo)\n",
    "        mu = np.sum(arr)\n",
    "        se = np.std(arr) * np.sqrt(len(arr))\n",
    "\n",
    "        ## Append.\n",
    "        loocv.append(dict(model=i+1, session=s, score='%0.1f' %score, loo='%0.1f' %loo, \n",
    "                          delta='%0.1f (%0.1f)' %(mu, se)))\n",
    "    \n",
    "## Convert to DataFrame.\n",
    "loocv = DataFrame(loocv).set_index(['session','model']).sort_index()\n",
    "loocv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009464ee-feaa-46e5-aa48-89b2165283ca",
   "metadata": {},
   "source": [
    "## Section 3: Posterior Predictive Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b432b8-64a3-42eb-9fd2-b4d5f41fc524",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define parameters.\n",
    "models = ['pgng_m1', 'pgng_m2', 'pgng_m3', 'pgng_m4', 'pgng_m5', 'pgng_m6', 'pgng_m7']\n",
    "sessions = ['s1', 's2', 's3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c6031-3ccd-4779-8431-fc857b6559f4",
   "metadata": {},
   "source": [
    "### 3.1 Group-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c1e8b-ff07-4fec-9372-0405b3d5eced",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize canvas.\n",
    "fig, axes = plt.subplots(len(sessions), len(models), figsize=(len(models)*4, len(sessions)*3),\n",
    "                         sharex=True, sharey=True)\n",
    "\n",
    "## Define aesthetics.\n",
    "order = ['gw', 'ngw', 'gal', 'ngal']\n",
    "palette = sns.diverging_palette(220, 20, n=4)\n",
    "\n",
    "for i, s in enumerate(sessions):\n",
    "    \n",
    "    for j, m in enumerate(models):\n",
    "        \n",
    "        ## Load posterior predictive check.\n",
    "        ppc = read_csv(os.path.join('stan_results', s, f'{m}_ppc.csv'))\n",
    "            \n",
    "        ## Plot learning curves.\n",
    "        sns.lineplot(x='exposure', y='choice', hue='robot', data=ppc, hue_order=order,\n",
    "                      palette=palette, lw=3, ci=None, ax=axes[i,j])\n",
    "        sns.lineplot(x='exposure', y='Y_hat', hue='robot', data=ppc, hue_order=order, \n",
    "                      palette=palette, lw=3, ci=None, linestyle='--', ax=axes[i,j])\n",
    "\n",
    "        ## Add trend line.\n",
    "        axes[i,j].axhline(0.5, color='0.5', alpha=0.4, zorder=-10)\n",
    "        \n",
    "        ## Adjust legend.\n",
    "        axes[i,j].legend_.set_visible(False)\n",
    "        \n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ddd8b1-b648-456d-bce5-b8435e64da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize canvas.\n",
    "fig, axes = plt.subplots(len(sessions), len(models), figsize=(len(models)*3, len(sessions)*3),\n",
    "                         sharex=True, sharey=True)\n",
    "\n",
    "## Define aesthetics.\n",
    "order = ['gw', 'ngw', 'gal', 'ngal']\n",
    "palette = sns.diverging_palette(220, 20, n=4)\n",
    "\n",
    "## Define convenience functions.\n",
    "RMSE = lambda x: np.sqrt(np.mean(np.square(x)))\n",
    "\n",
    "for i, s in enumerate(sessions):\n",
    "    \n",
    "    for j, m in enumerate(models):\n",
    "        \n",
    "        ## Load posterior predictive check.\n",
    "        ppc = read_csv(os.path.join('stan_results', s, f'{m}_ppc.csv'))\n",
    "\n",
    "        ## Compute accuracy by participant / condition.\n",
    "        gb = ppc.groupby(['subject','robot']).agg({'choice':'mean', 'Y_hat':'mean'}).reset_index()\n",
    "        \n",
    "        ## Compute fit statistics.\n",
    "        rmse = RMSE(gb.choice - gb.Y_hat)\n",
    "        corr = gb[['choice','Y_hat']].corr().values[0,1]\n",
    "        \n",
    "        ## Plot learning curves.\n",
    "        sns.scatterplot(x='choice', y='Y_hat', hue='robot', data=gb, hue_order=order, \n",
    "                        palette=palette, ax=axes[i,j])\n",
    "        axes[i,j].plot([-1,2], [-1,2], color='0.8')\n",
    "        \n",
    "        ## Adjust x-axis.\n",
    "        axes[i,j].set(xlim=(-0.05,1.05))\n",
    "        \n",
    "        ## Adjust y-axis\n",
    "        axes[i,j].set(ylim=(-0.05,1.05))\n",
    "        \n",
    "        ## Adjust legend.\n",
    "        axes[i,j].legend(loc=4, frameon=False, ncol=2, borderpad=0, handletextpad=0, columnspacing=0.3)\n",
    "        \n",
    "        ## Add annotation.\n",
    "        annot = 'RMSE = %0.3f\\nr = %0.3f' %(rmse, corr)\n",
    "        axes[i,j].annotate(annot, (0,0), (0.04, 0.98), 'axes fraction', ha='left', va='top', fontsize=11)\n",
    "        \n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72047e3f-ec45-4c1b-b5c6-fa54b380a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define parameters.\n",
    "sessions = ['s1', 's2', 's3']\n",
    "\n",
    "## Iteratively load data.\n",
    "data = concat([read_csv(os.path.join('stan_results', session, 'pgng_m7_ppc.csv'))\n",
    "               for session in ['s1','s2','s3']])\n",
    "data['exposure'] -= 1\n",
    "\n",
    "g = sns.FacetGrid(data, col='runsheet', col_order=['1a','2a','3a','1b','2b','3b'], col_wrap=3)\n",
    "palette = sns.diverging_palette(220, 20, n=4)\n",
    "\n",
    "g.map(sns.pointplot, 'exposure', 'choice', 'robot', order=np.arange(12),\n",
    "      hue_order=['gw','gal','ngw','ngal'], palette=palette, ci=None)\n",
    "g.map(sns.lineplot, 'exposure', 'Y_hat', 'robot', \n",
    "      hue_order=['gw','gal','ngw','ngal'], palette=palette, linestyle='--', ci=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38eb8a3-87df-4d80-b4f4-0244ed19c463",
   "metadata": {},
   "source": [
    "## Section 4: Parameter stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7907ccb4-1215-4978-bedf-df245c54c5eb",
   "metadata": {},
   "source": [
    "### 4.1 Between-session comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc59e1dd-2eb8-4334-8d6f-8ef69365a014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Mean[1]</th>\n",
       "      <th>Mean[2]</th>\n",
       "      <th>delta</th>\n",
       "      <th>lb</th>\n",
       "      <th>ub</th>\n",
       "      <th>credible</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">b1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">s1</th>\n",
       "      <th>s2</th>\n",
       "      <td>7.582</td>\n",
       "      <td>9.409</td>\n",
       "      <td>-1.827</td>\n",
       "      <td>-4.181</td>\n",
       "      <td>0.589</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s3</th>\n",
       "      <td>7.582</td>\n",
       "      <td>13.328</td>\n",
       "      <td>-5.746</td>\n",
       "      <td>-9.217</td>\n",
       "      <td>-2.520</td>\n",
       "      <td>**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <td>9.409</td>\n",
       "      <td>13.328</td>\n",
       "      <td>-3.918</td>\n",
       "      <td>-7.755</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">b2</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">s1</th>\n",
       "      <th>s2</th>\n",
       "      <td>6.626</td>\n",
       "      <td>11.093</td>\n",
       "      <td>-4.468</td>\n",
       "      <td>-7.548</td>\n",
       "      <td>-1.697</td>\n",
       "      <td>**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s3</th>\n",
       "      <td>6.626</td>\n",
       "      <td>13.054</td>\n",
       "      <td>-6.429</td>\n",
       "      <td>-10.281</td>\n",
       "      <td>-3.126</td>\n",
       "      <td>**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <td>11.093</td>\n",
       "      <td>13.054</td>\n",
       "      <td>-1.961</td>\n",
       "      <td>-6.239</td>\n",
       "      <td>2.066</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">b3</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">s1</th>\n",
       "      <th>s2</th>\n",
       "      <td>1.448</td>\n",
       "      <td>1.041</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.722</td>\n",
       "      <td>**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s3</th>\n",
       "      <td>1.448</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.781</td>\n",
       "      <td>**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <td>1.041</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>0.359</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">b4</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">s1</th>\n",
       "      <th>s2</th>\n",
       "      <td>0.182</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.284</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s3</th>\n",
       "      <td>0.182</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.201</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <td>0.105</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.135</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">a1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">s1</th>\n",
       "      <th>s2</th>\n",
       "      <td>0.316</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.100</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s3</th>\n",
       "      <td>0.316</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.209</td>\n",
       "      <td>**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <td>0.300</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.184</td>\n",
       "      <td>**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">a2</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">s1</th>\n",
       "      <th>s2</th>\n",
       "      <td>0.408</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.298</td>\n",
       "      <td>**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s3</th>\n",
       "      <td>0.408</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.319</td>\n",
       "      <td>**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <td>0.215</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.115</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">c1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">s1</th>\n",
       "      <th>s2</th>\n",
       "      <td>0.083</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.078</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s3</th>\n",
       "      <td>0.083</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.063</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.033</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Mean[1]  Mean[2]  delta      lb     ub credible\n",
       "param s1 s2                                                 \n",
       "b1    s1 s2    7.582    9.409 -1.827  -4.181  0.589         \n",
       "         s3    7.582   13.328 -5.746  -9.217 -2.520       **\n",
       "      s2 s3    9.409   13.328 -3.918  -7.755 -0.458       **\n",
       "b2    s1 s2    6.626   11.093 -4.468  -7.548 -1.697       **\n",
       "         s3    6.626   13.054 -6.429 -10.281 -3.126       **\n",
       "      s2 s3   11.093   13.054 -1.961  -6.239  2.066         \n",
       "b3    s1 s2    1.448    1.041  0.407   0.083  0.722       **\n",
       "         s3    1.448    0.982  0.466   0.151  0.781       **\n",
       "      s2 s3    1.041    0.982  0.058  -0.262  0.359         \n",
       "b4    s1 s2    0.182    0.105  0.077  -0.110  0.284         \n",
       "         s3    0.182    0.192 -0.010  -0.211  0.201         \n",
       "      s2 s3    0.105    0.192 -0.087  -0.290  0.135         \n",
       "a1    s1 s2    0.316    0.300  0.016  -0.064  0.100         \n",
       "         s3    0.316    0.194  0.122   0.045  0.209       **\n",
       "      s2 s3    0.300    0.194  0.106   0.027  0.184       **\n",
       "a2    s1 s2    0.408    0.215  0.193   0.084  0.298       **\n",
       "         s3    0.408    0.196  0.212   0.102  0.319       **\n",
       "      s2 s3    0.215    0.196  0.019  -0.070  0.115         \n",
       "c1    s1 s2    0.083    0.052  0.031  -0.021  0.078         \n",
       "         s3    0.083    0.072  0.011  -0.043  0.063         \n",
       "      s2 s3    0.052    0.072 -0.020  -0.068  0.033         "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define parameters.\n",
    "pairs = list(combinations(['s1','s2','s3'], 2))\n",
    "params = ['b1','b2','b3','b4','a1','a2','c1']\n",
    "model = 'pgng_m7'\n",
    "\n",
    "## Main loop.\n",
    "comparisons = []\n",
    "for s1, s2 in pairs:\n",
    "    \n",
    "    ## Load samples.\n",
    "    samples_1 = read_csv(os.path.join('stan_results', s1, f'{model}.tsv.gz'), \n",
    "                                      sep='\\t', compression='gzip')\n",
    "    samples_2 = read_csv(os.path.join('stan_results', s2, f'{model}.tsv.gz'), \n",
    "                         sep='\\t', compression='gzip')\n",
    "    \n",
    "    ## Iterate over parameters.\n",
    "    for p in params:\n",
    "        \n",
    "        ## Extract parameters.\n",
    "        a = samples_1[f'{p}_mu'].values\n",
    "        b = samples_2[f'{p}_mu'].values\n",
    "        \n",
    "        ## Summarize & report.\n",
    "        mu1 = np.mean(a); mu2 = np.mean(b); delta = np.mean(a - b)\n",
    "        lb, ub = hdi(a - b, hdi_prob=0.95)\n",
    "        is_credible = '**' if np.sign(lb) == np.sign(ub) else ''\n",
    "        comparisons.append({'s1': s1, 's2': s2, 'param': p, 'Mean[1]': mu1, 'Mean[2]': mu2, \n",
    "                            'delta': delta, 'lb': lb, 'ub': ub, 'credible': is_credible})\n",
    "        \n",
    "## Convert to DataFrame.\n",
    "comparisons = DataFrame(comparisons).set_index(['param','s1','s2']).sort_index()\n",
    "comparisons.loc[params].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7a5954-cbaa-40d7-88cf-263e2403b686",
   "metadata": {},
   "source": [
    "## Section 6: Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88556f3-22ed-43a7-9ead-198e43cb0c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define paramaters.\n",
    "model = 'pgng_m7'\n",
    "\n",
    "## Load summary.\n",
    "reliability = read_csv(os.path.join('stan_results', f'{model}_reliability.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d3760-5752-4e71-8bd3-e2675f65c619",
   "metadata": {},
   "source": [
    "### 6.1 Split-half reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d81615-c0b9-4862-85a3-9a0f58985981",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize canvas.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "palette = np.append('k', sns.color_palette('crest_r', n_colors=3).as_hex())\n",
    "labels = ['Overall','Session 1','Session 2', 'Session 3']\n",
    "offsets = np.linspace(-0.2,0.2,4)\n",
    "\n",
    "for i, (offset, color, label) in enumerate(zip(offsets, palette, labels)):\n",
    "\n",
    "    ## Define points.\n",
    "    y = reliability.query(f'Type == \"sh\" and Group == {i}').Mean\n",
    "    x = np.arange(len(y)) + offset\n",
    "    yerr = np.array([\n",
    "        reliability.query(f'Type == \"sh\" and Group == {i}')['97.5%'] - y,\n",
    "        y - reliability.query(f'Type == \"sh\" and Group == {i}')['2.5%']\n",
    "    ])\n",
    "    \n",
    "    ## Plot.\n",
    "    ax.errorbar(x, y, fmt='o', yerr=yerr, color=color, label=label, capsize=3, elinewidth=1.33)\n",
    "    \n",
    "## Add detail.\n",
    "ax.axhline(0.7, color='0.8', lw=0.8, linestyle='--')\n",
    "ax.legend(loc=4, frameon=False, borderpad=0, handletextpad=0.2)\n",
    "ax.set(xticks=np.arange(7), ylim=(0,1.05), ylabel='Split-half reliability')\n",
    "ax.set_xticklabels(['Inverse\\ntemperature\\n(Positive)','Inverse\\ntemperature\\n(Negative)',\n",
    "                    'Go Bias\\n(Positive)','Go Bias\\n(Negative)','Learning\\nRate\\n(Positive)',\n",
    "                    'Learning\\nRate\\n(Negative)','Lapse Rate'])\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1ddd4-2fbb-414b-bba9-deb936159e5f",
   "metadata": {},
   "source": [
    "### 6.2 Test-retest reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3624340-b340-46a6-972f-c72e36cb7d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize canvas.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "palette = np.append('k', sns.color_palette('crest_r', n_colors=3).as_hex())\n",
    "labels = ['Overall','S1 vs. S2','S1 vs. S3', 'S2 vs. S3']\n",
    "offsets = np.linspace(-0.2,0.2,4)\n",
    "\n",
    "for i, (offset, color, label) in enumerate(zip(offsets, palette, labels)):\n",
    "\n",
    "    ## Define query.\n",
    "    query = f'Type == \"trt\" and Group == {i}'\n",
    "    \n",
    "    ## Define points.\n",
    "    y = reliability.query(query).Mean\n",
    "    x = np.arange(len(y)) + offset\n",
    "    yerr = np.array([\n",
    "        reliability.query(query)['97.5%'] - y,\n",
    "        y - reliability.query(query)['2.5%']\n",
    "    ])\n",
    "    \n",
    "    ## Plot.\n",
    "    ax.errorbar(x, y, fmt='o', yerr=yerr, color=color, label=label, capsize=3, elinewidth=1.33)\n",
    "    \n",
    "## Add detail.\n",
    "ax.axhline(0.7, color='0.8', lw=0.8, linestyle='--')\n",
    "ax.legend(loc=4, frameon=False, borderpad=0, handletextpad=0.2)\n",
    "ax.set(xticks=np.arange(7), ylim=(0,1.05), ylabel='Test-retest reliability')\n",
    "ax.set_xticklabels(['Inverse\\ntemperature\\n(Positive)','Inverse\\ntemperature\\n(Negative)',\n",
    "                    'Go Bias\\n(Positive)','Go Bias\\n(Negative)','Learning\\nRate\\n(Positive)',\n",
    "                    'Learning\\nRate\\n(Negative)','Lapse Rate'])\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
