% set document class
\documentclass[a4paper,12pt]{article}

% load packages
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=0.8in]{geometry}
\usepackage[english]{babel}
\usepackage{authblk}
\usepackage[backend=biber,style=nature]{biblatex}    % citation management
\usepackage[font=small, labelfont=it]{caption}       % figure/table captions
\usepackage{float, graphicx, svg}                    % figure/table management
\usepackage[pagewise]{lineno}                        % add line numbers for reviewers
\usepackage{indentfirst}                             % for pretty paragraphs
\usepackage{booktabs}                                % for pretty tables
\usepackage{hyperref}                                % for pretty links
\usepackage{amsmath}                                 % for pretty equations
\usepackage[export]{adjustbox}                       % center wide tables on page

% setup references
\defbibheading{main}{}
\defbibheading{supp}{}
\addbibresource[label=main]{main.bib}
\addbibresource[label=supp]{supp.bib}
\renewcommand*{\bibfont}{\small}

% specify link style
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue
}

% set up author block
\renewcommand{\baselinestretch}{1.2}
\renewcommand*{\Authsep}{, }
\renewcommand*{\Authand}{, }
\renewcommand*{\Authands}{, }
\renewcommand*{\Affilfont}{\normalsize}

\setlength{\affilsep}{2em}   % set the space between author and affiliation
\author[1,*]{Samuel Zorowitz}
\author[1]{Gili Karni}
\author[2]{Natalie Paredes}
\author[1,3]{Nathaniel Daw}
\author[1,3]{Yael Niv}
\affil[1]{Princeton Neuroscience Institute, Princeton University, USA}
\affil[2]{Department of Psychology, University of California, San Diego, USA}
\affil[3]{Department of Psychology, Princeton University, USA}
\affil[*]{Corresponding author (szorowi1@gmail.com)}

% specify title / suppress date
\title{Improving the Reliability of the Pavlovian \break Go/No-Go Task}
\date{}

\begin{document}

% title page
\maketitle
\thispagestyle{empty}          % remove page number
{\bf Keywords:} Pavlovian go/no-go task; Pavlovian bias; reinforcement learning; reliability

% abstract page
\break

%TC:ignore
\abstract{

\noindent \textbf{Background:} The Pavlovian go/no-go task is commonly used to measure individual differences in Pavlovian biases and their interaction with instrumental learning. However, prior research has reported suboptimal reliability for computational model-based performance measures for this task, limiting its usefulness in individual-differences research. These studies did not make use of several strategies previously shown to enhance task-measure reliability (e.g., task gamification, hierarchical Bayesian modeling for model estimation). Here we investigated if such approaches could improve the task's reliability.\\

\noindent \textbf{Methods:} Across two experiments, we recruited two independent samples of adult participants (N=103, N=110) to complete a novel, gamified version of the Pavlovian go/no-go task multiple times over several weeks. We used hierarchical Bayesian modeling to derive reinforcement learning model-based indices of participants' task performance, and additionally to estimate the reliability of these measures.\\

\noindent \textbf{Results:} In Experiment 1, we observed considerable and unexpected practice effects, with most participants reaching near-ceiling levels of performance with repeat testing. Consequently, the test-retest reliability of some model parameters was unacceptable (range: 0.379--0.973). In Experiment 2, participants completed a modified version of the task designed to lessen these practice effects. We observed greatly reduced practice effects and improved estimates of the test-retest reliability (range: 0.696--0.989).\\

\noindent \textbf{Conclusion:} The results demonstrate that model-based measures of performance on the Pavlovian go/no-go task can reach levels of reliability sufficient for use in individual-differences research. However, additional investigation is necessary to validate the modified version of the task in other populations and settings.
}
\thispagestyle{empty}          % remove page number
%TC:endignore

% manuscript start
\break
% \linenumbers                 % toggle to add line numbers
\pagenumbering{arabic}         % reset page numbers
\setlength{\parindent}{0em}    % remove paragraph indenting
\setlength{\parskip}{1em}      % increase space between paragraphs
\begin{refsection}[main]       % start manuscript reference section

\section*{Introduction}

Humans (and other animals) have an innate tendency to approach rewarding stimuli and shrink from punishing stimuli \cite{carver1994behavioral}. Depending on the context, these hardwired Pavlovian biases can either benefit or interfere with instrumental (i.e., action-outcome) learning. This is epitomized in the Pavlovian go/no-go task in which the required action (Go, No-Go) and outcome valence (reward, punishment) are orthogonalized \cite{guitart2012go, guitart2014action}. In the task, participants are typically faster to learn actions that are congruent with Pavlovian response biases (i.e., initiate action to receive reward, inhibit action to avoid punishment) and slower to learn Pavlovian-instrumental incongruent responses (i.e., inhibit action to receive reward, initiate action to avoid punishment). %These biases can arise during action selection \cite{guitart2012go} and learning (i.e., increased sensitivity to rewards and punishments following action and inaction, respectively; \cite{swart2017catecholaminergic}). 

The Pavlovian go/no-go task has been used in innumerable studies to probe individual-differences in reward and punishment learning, of which many have reported changes in Pavlovian biases as a function of psychiatric conditions. For example, an increased tendency towards passive avoidance has been observed in individuals with general and social anxiety \cite{mkrtchian2017modeling, peterburs2021impact}, whereas active avoidance is amplified in individuals with a history of suicidal thoughts or behaviors \cite{millner2019suicidal}. Pavlovian biases are larger in individuals with trauma exposure \cite{ousdal2018impact} and first-episode psychosis \cite{montagnese2020reinforcement}, but attenuated in individuals with depression \cite{huys2016specificity} and schizophrenia \cite{albrecht2016reduction}. Pavlovian biases have also been associated with individual differences in personality (e.g., impulsivity; \cite{eisinger2020pavlovian}) and genetics \cite{richter2014valenced, richter2021motivational}. In developmental and lifespan research, Pavlovian biases have been shown to exhibit a U-shape, decreasing from childhood to young adulthood and increasing again in older age \cite{raab2020adolescents, betts2020learning}. At a finer temporal scale, Pavlovian biases are also reportedly modulated by state effects including mood \cite{weber2022effects}, anger \cite{wonderlich2020anger}, stress \cite{de2016acute}, and fear \cite{mkrtchian2017threat}. 

Despite its extensive use, the Pavlovian go/no-go task has received less psychometric investigation. What studies exist suggest the task, in its canonical form, is suboptimal for use in individual-differences correlational research. Specifically, two independent studies found that descriptive and computational model-based measures of performance on the Pavlovian go/no-go task exhibited suboptiomal test-retest reliability over short (two-week) and long (6-, 18-month) retest intervals \cite{moutoussis2018change, pike2022test}. This is important because the reliability of a measure places an upper bound on the maximum observable correlation between itself and a second measure (e.g., symptom scores; \cite{Spearman1904-mo}). Therefore, as reliability decreases, so too does statistical power; in turn, this increases the possibility of false-negatives \cite{Parsons2019-jw} and bias in correlations that do reach statistical significance \cite{gelman2014beyond}. One of these studies also reported evidence of practice effects (i.e., improved performance and diminished Pavlovian biases with repeated testing; \cite{moutoussis2018change}). Practice effects are of additional concern for individual-differences research because they can imperil reliability (e.g., by minimizing between-participants variability) and obscure effects of interest (e.g., changes in task performance following changes in mood).

These previous studies of the psychometrics of the Pavlovian go/no-go task, however, did not make use of multiple strategies for improving the reliability of cognitive task measures \cite{zorowitz2023improving}. For example, prior research has found that gamification, or the incorporation of (video) game design elements into cognitive tasks, can promote participant engagement \cite{sailer2017gamification} and improve the reliability of task measures \cite{kucina2022solution, verdejo2021unified}. Moreover, the use of hierarchical Bayesian models --- which exert a pooling effect on person-level variables, in effect correcting them for measurement error \cite{haines2023classical, rouder2019psychometrics} --- have been frequently shown to improve the reliability of task measures \cite{sullivan2022enhancing, brown2020improving, waltmann2022sufficient}. Finally, practice effects can be lessened by designing tasks in such a way that prevents participants from discovering and using task-specific knowledge to enhance their performance on subsequent attempts \cite{mclean2018towards}. 

The aim of the current study was to investigate the reliability and repeatability of a novel version of the Pavlovian go/no-go task. We conducted two experiments involving two independent samples of adult participants who completed a gamified version of the task multiple times over several weeks. We used hierarchical Bayesian models to derive reinforcement learning model-based indices of their task performance, and additionally to estimate the reliability of these measures. In Experiment 1, participants unexpectedly exhibited large practice effects, which negatively impacted the test-retest reliability of the performance measures. To address this issue, in Experiment 2, participants completed a modified version of the task that reduced practice effects, and led to significant improvements in the test-retest reliability of the reinforcement learning model parameters.

\section*{Experiment 1}

\subsection*{Methods}

\subsubsection*{Participants}

A total of N=148 participants were recruited in May, 2020 from Amazon Mechanical Turk via CloudResearch \cite{litman2017turkprime} to participate in an online behavioral experiment. Participants were eligible to participate if they were at least 18 years old and resided in the United States. Following best practice recommendations \cite{robinson2019tapped}, no other inclusion criteria were applied. This study was approved by the Institutional Review Board of Princeton University and all participants provided informed consent. Total study duration was 15--20 minutes. Participants received monetary compensation for their time (rate: USD \$12/hr), plus an incentive-compatible bonus up to \$1.50 based on task performance. 

Data from N=45 participants who completed the first session were excluded prior to analysis (see ``Exclusion criteria'' below), leaving a final sample of N=103 participants. These participants were re-invited to complete a follow-up experiment 3, 14, and 28 days later. Once invited, participants were permitted 48 hours to complete the follow-up experiment. Retention was high for each follow-up session (Day 3: N=94 [91.3\%]; Day 14: N=92 [89.3\%]; Day 28: N=89 [86.4\%]). In addition to the performance bonus, participants received a retention bonus of \$1.00 for each completed follow-up session. Detailed demographic information is presented in Table \ref{tab:demographics}. The majority of participants identified as men (55 men; 47 women; 1 non-binary) and were 35.5 years old on average (range: 20--69 years).

\subsubsection*{Experimental protocol}

In each session, after providing consent, participants started by completing several self-report questionnaires (see Supplementary Materials for details). These measures were included for exploratory analyses not reported here. Next, participants completed a gamified version of the Pavlovian go/no-go task. In the task, participants observed different `robot' stimuli (Figure \ref{fig:task_schematic}A). On every trial, a robot was shown traveling down a conveyer belt into a `scanner'. Once inside, participants had 1.5 seconds to decide to either `repair' the robot by pressing the space bar (Go response) or press nothing (No-Go response). Participants were told that they would see different types of robots (indicated by a symbol on the robots' chestplates), and that their goal was to learn the correct response (Go, No-Go) for each robot type based on feedback (points won/lost) following their actions.

The task involved four trial types that differed by their correct action (Go, No-Go) and outcome domain (reward, punishment; Figure \ref{fig:task_schematic}B). Specifically, the four trial types were: go to win points (GW); no-go to win points (NGW); go to avoid losing points (GAL); and no-go to avoid losing points (NGAL). Note that GW and NGAL trials are Pavlovian-instrumental `congruent' because there is a match between the correct response and the expected approach/avoidance bias for each. In contrast, NGW and GAL trials are Pavlovian-instrumental `incongruent'. For the rewarding trials (GW, NGW), the possible outcomes were +10 or +1 points; for the punishing trials (GAL, NGAL), they were -1 or -10 points. The outcome domain of each robot was explicitly signaled to participants by a blue or orange `scanner light' (randomized within participants across sessions). Outcomes were probabilistic such that participants received the better of the two possible outcomes with 80\% chance if they made the correct action, and received the worse of the possible outcomes with 80\% chance if they made the incorrect action (Figure \ref{fig:task_schematic}C).

Participants saw eight unique robots across the task. Each individual robot was presented for 30 trials (240 trials total; Figure \ref{fig:task_schematic}D). Trials were divided into two blocks with four robots (one of each trial type) per block. Prior to task start, participants were required to review instructions, correctly answer five comprehension questions, and complete several practice trials. Failing to correctly answer all comprehension items forced the participant to reread sections of the instructions. Participants were provided a break between blocks. After completing the task, participants appraised the task along three dimensions: difficulty, fun, and clarity of instructions (see Table \ref{tab:appraisals}). The task was programmed in jsPsych \cite{de2015jspsych} and distributed using custom web-application software (see Code Availability). 

%TC:ignore
\begin{figure}[t]
    \centerline{\includesvg[inkscapelatex=false,width=1.00\textwidth]{figures/fig01.svg}}
    \caption{(A) Schematic of the Pavlovian go/no-go task. On each trial, a robot entered the `scanner' from the left of screen, prompting a response (go or no-go) from the participant during a response window (Experiment 1: 1.5 seconds; Experiment 2: 1.3 seconds). The outcome (number of points won or lost) was subsequently presented (Experiment 1: 1.0 seconds; Experiment 2: 1.2 seconds), followed by an inter-trial interval animation (1 second) in which the conveyer belt carried the old robot out of view and a new robot into the scanner. The color of the scanner light denotes outcome domain (e.g., blue denotes reward and red denotes punishment). (B) The four trial types, produced by a factorial combination of outcome domain (rewarding, punishing) and correct action (go, no-go). C) Outcome probabilities for each outcome domain following a correct or incorrect response. Correct responses yielded the better of the two possible outcomes with 80\% chance. (D) Trial composition. In Experiment 1, participants saw 8 total robots (two of each trial type), each presented for 30 trials (240 total trials). In Experiment 2, participants saw 24 total robots (6 of each trial type), each between for 8, 10, or 12 trials (240 total trials).}
    \label{fig:task_schematic}
\end{figure}
%TC:endignore

\subsubsection*{Exclusion criteria}

To ensure data quality, the data from multiple participants from the initial session were excluded prior to analysis. In Experiment 1, the data from multiple participants who completed the experiment on Day 0 were excluded prior to analysis for one or both of the following reasons: failing more than one attention check embedded in the self-report measures (N=13; \cite{zorowitz2023inattentive}) or exhibiting chance-level performance ($<$55\% correct responses) on go to win trials (N=43). Data from N=45 participants who completed the first session were excluded based on these criteria, leaving a final sample of N=103 participants. No exclusions were applied to subsequent session data. 

\subsubsection*{Descriptive analyses}

We first evaluated participants' choice behavior using five performance measures: overall percent correct responses; go bias, or the difference in correct responses between Go and No-Go trials; outcome bias, or the difference in correct responses between rewarding and punishing trials; Pavlovian bias, or the difference in correct responses between Pavlovian-instrumental congruent and incongruent trials; and feedback sensitivity, or the difference in correct responses between trials following veridical or sham feedback. For each session and measure, we tested if the median value across participants was significantly different than zero (or 50\% for overall percent correct responses). We used the median due to skew in the performance measures. We also tested if the median value of each measure was significantly different between each pair of sessions. P-values were derived via permutation testing, where a null distribution of values were obtained by permuting the condition labels (for within-session tests) or session labels (for between-session tests) 5,000 times. Within-session tests were not corrected for multiple comparisons as each test constitutes an individual hypothesis test; however, between-session tests were corrected, using the family-wise error rate correction \cite{winkler2014permutation}, because they constitute a disjunctive test \cite{rubin2021adjust}.

\subsubsection*{Reinforcement learning models}

We fit a set of seven nested reinforcement learning (RL) models to participants' choice data. All models are variants of the Rescorla-Wagner model and have previously been used to predict choice behavior on this task \cite{guitart2012go, mkrtchian2017modeling, moutoussis2018change, swart2017catecholaminergic}. In the base model (M1), the probability a participant chooses an action in response to stimulus $k$ is determined by the difference in stimulus-action values ($Q_k(\text{Go}) - Q_k(\text{No-Go}))$, scaled an outcome sensitivity parameter ($\beta$), and passed through the logistic function. In turn, stimulus-action values are learned via the Rescorla-Wagner update rule as governed by a learning rate ($\eta$). Model 2 is augmented with a single static approach/avoidance parameter ($\tau$) that biases action towards Go responses if $\tau_j > 0$ and No-Go responses if $\tau < 0$. Model 3 (M3) introduces independent approach/avoidance parameters per outcome domain ($\tau_+$, $\tau_-$). Models 4 and 5 have either independent outcome sensitivity ($\beta_+$, $\beta_-$; M4) or learning rate parameters ($\eta_+$, $\eta_-$; M5) per outcome domain. In Model 6, all three parameter types ($\beta$, $\tau$, $\eta$) are independent across outcome domains. Finally, the most complex model (M7) includes a lapse rate ($\xi$). Complete model equations and descriptions are provided in the Supplementary Materials.

All models were estimated within a hierarchical Bayesian modeling framework using Hamiltonian Monte Carlo as implemented in Stan (v2.30; \cite{carpenter2017stan}). See the Supplementary Materials for details about model estimation and our choices of prior. Briefly, we specified diffuse and uninformative priors to avoid biasing parameter estimation.

The fits of the models to the data were assessed using posterior predictive checks. Specifically, we inspected each model's ability to reproduce both group-averaged learning curves (by trial type) and each participant's proportion of go responses (by trial type). Model fits were compared using approximate leave-one-out cross-validation via Pareto smoothed importance sampling (PSIS-LOO; \cite{vehtari2017practical}). A credible improvement in model fit was defined as a difference in PSIS-LOO values four times larger than its corresponding standard error \cite{Vehtari_undated-tc}.

We investigated the reliability of the model parameters for the best-fitting model also using a Bayesian hierarchical modeling framework \cite{rouder2019psychometrics}. Briefly, the best-fitting model was fit to participants' choice data with partial pooling over blocks within a session (for split-half reliability) or over whole datasets between sessions (for test-retest reliability). Complete details of the estimation procedure are reported in the Supplementary Materials. Split-half and test-retest reliability was then respectively calculated as the Pearson correlation between model parameters estimated from each block and each pair of sessions \cite{brown2020improving, pike2022test}. We use the Pearson correlation because we are primarily interested in the consistency of rank-ordering of participants' parameter estimates over time. Though arbitrary, we follow convention and define $\rho \geq 0.7$ as the threshold for ``acceptable'' reliability \cite{cicchetti1994guidelines}.  

\subsection*{Results}

\subsubsection*{Descriptive analyses}

%TC:ignore
\begin{figure}[hpt]
    \centerline{\includesvg[inkscapelatex=false,width=1.00\textwidth]{figures/fig02.svg}}
    \caption{Participants in Experiment 1 show large practice effects on the standard Pavlovian go/no-go task. (A) Group-averaged learning curves for each trial type and session. Shaded regions indicate 95\% bootstrapped confidence intervals. (B) Group-averaged performance for each session. Performance measures from left-to-right: Correct responses, or overall accuracy; Go bias, or difference in accuracy between Go and No-Go trials; Congruency effect, or difference in accuracy between Pavlovian congruent (GW, NGAL) and incongruent (NGW, GAL) trials; and Feedback sensitivity, or the difference in accuracy on trials following veridical and sham feedback. **Denotes significant pairwise difference ($p < 0.05$, corrected for multiple comparisons). (C) The percentage of participants, for each session and trial type, exhibiting at- or below-chance performance ($< 60\%$ response accuracy; grey), intermediate performance ($\geq 60\%$ response accuracy; light blue), or near-perfect performance ($\geq 90\%$ response accuracy; dark blue).}
    \label{fig:exp01_behavior}
\end{figure}
%TC:endignore

Trial-by-trial choice behavior for each session is presented in Figure \ref{fig:exp01_behavior}A. Performance in the first session qualitatively conformed to the expected pattern of results (i.e., worse performance on Pavlovian-instrumental incongruent trials). However, this effect was seemingly diminished in all follow-up sessions. We turn to the descriptive analyses to formally test these trends. 

The group-averaged performance measures by session are summarized in Figure \ref{fig:exp01_behavior}B. (Complete descriptive statistics are reported in Table~\ref{tab:stats_exp01}.) Participants made the correct response on 85.0\% of trials on Day 0, which increased to near-ceiling levels in all subsequent sessions. Pairwise comparisons confirmed that performance was indeed worse on Day 0 compared to each follow-up session (all $p < 0.001$); no other comparisons were significant. Participants' self-reported mood and anxiety were largely stable over the same period (Figure \ref{fig:figS05}), indicating this shift in performance more likely reflects practice effects rather than changes in participants' state. 

Across sessions, participants made more correct responses on Go trials than No-Go trials. However, the go bias was significantly reduced in all follow-up sessions compared to Day 0 (all $p < 0.001$); so too was it on Day 28 compared to Day 3 ($p < 0.001$). Similarly, participants made more correct responses on Pavlovian-congruent than incongruent trials. As with the Go bias, the Pavlovian bias was significantly reduced in all follow-up sessions compared to Day 0 (all $p < 0.001$); no other comparisons were significant. Regarding the valence bias, participants made marginally but significantly more correct responses on rewarding trials on Day 3 only; no significant difference were observed between sessions.

A similar pattern of results was observed for feedback sensitivity. Across sessions, participants made more correct responses following veridical compared to sham feedback (all $p < 0.001$). However, feedback sensitivity was significantly reduced in all follow-up sessions compared to Day 0 (all $p < 0.001$); no other comparisons were significant. This is consistent with participants' learning curves which show, excluding Day 0, participants quickly learn the correct action for each stimulus and maintain this policy despite 20\% sham feedback.

The results so far summarize group-averaged performance but do not provide much insight into individual differences. As such, the proportion of participants who exhibited chance-level ($<$60\% correct responses), intermediate ($\geq$60\%), or near-ceiling performance ($\geq$90\%) by session and trial type is presented in Figure \ref{fig:exp01_behavior}C. Excepting GW trials, the percentage of participants nearing ceiling-level performance increases from the minority on Day 0 to the majority in all follow-up sessions. Two-way chi-squared tests confirm this trend (GW: $\chi^2 (6) = 8.149$, $p = 0.227$; NGW: $\chi^2 (6) = 55.458$, $p < 0.001$; GAL: $\chi^2 (6) = 42.191$, $p < 0.001$; NGAL: $\chi^2 (6) = 39.287$, $p < 0.001$). In sum, the improvements in task performance (and accompanying reductions in choice biases) with repeat testing observed at the group-level extends to the majority of participants. 

\subsubsection*{Model comparison}

The results of the model comparison are summarized in Table \ref{tab:exp1_mc_abbr}. Collapsing across sessions, the best-fitting model was the most complex one (i.e., independent parameters per outcome domain plus lapse rate; M7). Importantly, this was also the best-fitting model within each session (Table \ref{tab:exp1_mc_full}). Posterior predictive checks indicated that this model provided excellent fits to the choice data from each session (Figure \ref{fig:exp1_ppc}). 

%TC:ignore
\begin{table}[b!]
    \centering
    \begin{tabular}{clccr}
        \toprule
        Model & Parameters & Acc. (\%) & PSIS-LOO & $\Delta$ PSIS-LOO (se) \\
        \midrule
        1 & $\beta, \eta$ & 87.5 & -151457.9 & -5602.6 (68.3) \\
        2 & $\beta, \tau, \eta$ & 89.0 & -154011.9 & -3048.6 (51.2) \\
        3 & $\beta, \tau_+, \tau_-, \eta$ & 89.8 & -155817.8 & -1242.7 (31.3) \\
        4 & $\beta_+, \beta_-, \tau_+, \tau_-, \eta$ & 89.8 & -156261.6 & -798.8 (22.6) \\
        5 & $\beta, \tau_+, \tau_-, \eta_+, \eta_-$ & 89.9 & -156265.9 & -794.6 (20.7) \\
        6 & $\beta_+, \beta_- \tau_+, \tau_-, \eta_+, \eta_-$ & 89.9 & -156401.8 & -658.6 (18.8) \\
        7 & $\beta_+, \beta_- \tau_+, \tau_-, \eta_+, \eta_-, \xi$ & 90.1 & -157060.5 & \multicolumn{1}{c}{-} \\
        \bottomrule
\end{tabular}
    \caption{Model comparison collapsing across sessions. Notes: Acc. = trial-level choice prediction accuracy between observed and model-predicted Go responses. PSIS-LOO = approximate leave-one-out cross-validation scores presented in deviance scale (i.e., smaller numbers indicate better fit). \break $\Delta$PSIS-LOO = difference in PSIS-LOO values between each model and the best-fitting model (M7).}
    \label{tab:exp1_mc_abbr}
\end{table}
%TC:endignore

\subsubsection*{Model parameters}

The estimated group-level parameters from the best-fitting model are presented in Figure \ref{fig:exp01_modeling}A. Consistent with the descriptive analyses, large shifts in the parameters were observed following Day 0. The reward and punishment sensitivity parameters ($\beta_+$, $\beta_-$) exhibited an almost threefold increase between Days 0 and 3, and stabilized thereafter. The inverse pattern was observed for the positive learning rate ($\eta_+$). Crucially, the approach and avoidance bias parameters followed a similar pattern. The approach bias ($\tau_+$) credibly decreased between Days 0 and 3, and qualitatively declined thereafter. In turn, the avoidance bias ($\tau_-$) credibly increased between Days 0 and 3, but stabilized thereafter. That is, Pavlovian biases diminished in absolute and relative (i.e., compared to the outcome sensitivity parameters) terms with repeat testing. 

The test-retest reliability estimates for each model parameter is presented in Figure \ref{fig:exp01_modeling}B. The results are mixed. Averaging across session-pairs, acceptable test-retest reliability was observed for the reward sensitivity ($\bar{\rho} = 0.863$, 95\% CI = [0.837, 0.889]), punishment sensitivity ($\bar{\rho} = 0.973$, 95\% CI = [0.963, 0.981]), and negative learning rate ($\bar{\rho} = 0.846$, 95\% CI = [0.764, 0.886]) parameters. Conversely, unacceptable test-retest reliability was observed for the approach bias ($\bar{\rho} = 0.379$, 95\% CI = [0.270, 0.486]), avoidance bias ($\bar{\rho} = 0.502$, 95\% CI = [0.412, 0.583]), and positive learning rate ($\bar{\rho} = 0.446$, 95\% CI = [0.327, 0.555]) parameters. A similarly mixed pattern was observed for the split-half reliability estimates (Figure \ref{fig:figS04}A).

%TC:ignore
\begin{figure}[t!]
    \centerline{\includesvg[inkscapelatex=false,width=1.00\textwidth]{figures/fig03.svg}}
    \caption{Reinforcement learning model parameters in Experiment 1 mostly show evidence of practice effects and suboptimal reliability. (A) Group-level model parameters for each session. Error bars indicate 95\% HDIs. **Denotes pairwise comparison where 95\% HDI of the difference excludes zero. (B) Test-retest reliability estimates for each model parameter. Dotted lines indicates overall average. Shaded region indicates conventional range of acceptable reliability ($\rho \geq 0.7$).}
    \label{fig:exp01_modeling}
\end{figure}
%TC:endignore

\subsection*{Discussion}

The objectives of this first experiment was to evaluate the stability and reliability of individual differences in performance on a gamified version of the Pavlovian go/no-go task. At both the group- and participant-levels, we observed significant practice effects on performance following the initial session. An increasing majority of participants exhibited near-ceiling performance, across trial types, with each additional task administration. Consequently, the magnitude of group-averaged behavioral effects including the go bias, Pavlovian bias, and feedback sensitivity were diminished by half or more after the first session. This was reflected in the group-level parameters of a reinforcement learning model fit to participants' choice data, which indicated that Pavlovian biases were credibly attenuated in follow-up sessions. Relatedly, we found that the Pavlovian bias parameters exhibited poor-to-moderate test-retest reliability. This last result is perhaps unsurprising insofar that low between-participants variability decreases reliability \cite{zorowitz2023improving}.  

The results of Experiment 1 raise two questions: what underlies these practice effects and what can be done to mitigate or prevent them? With respect to the first question, one possibility is that, after the initial session, participants employ an alternative strategy to complete the task. Consider that there is an implicit dependence between stimuli in the canonical Pavlovian go/no-go task: namely, for every Go stimulus (e.g., GW) there is always a corresponding No-Go stimulus (e.g., NGW). That is, learning the correct action for one stimulus provides information about the correct action for its complement. Recognizing this, savvy participants may forego reinforcement learning in favor of a process-of-elimination strategy and thereby improve their performance on the task. Indeed, feedback from several participants in this study suggested that they may have utilized this form of top-down strategy.  

This suggests that, in order to mitigate or prevent practice effects, there is a need for a modified version of the Pavlovian go/no-go task with a less predictable trial structure. By eliminating the dependence between stimuli, motivated participants aiming to maximize their performance should have no strategy better than simply paying attention to each and every stimulus equally. Furthermore, by minimizing practice effects and increasing between-participants variability, it is plausible that parameter reliability would also improve. In the next experiment, we investigated precisely this.

\section*{Experiment 2}

\subsection*{Methods}

\subsubsection*{Participants}

A total of N=156 participants were recruited in December, 2020 from Amazon Mechanical Turk via CloudResearch \cite{litman2017turkprime} to participate in an online behavioral experiment. The inclusion criteria were the same as in Experiment 1. This study was approved by the Institutional Review Board of Princeton University, and all participants provided informed consent. Total study duration was again 15-20 minutes. Monetary compensation, including the performance bonus, was the same as in Experiment 1. 

Data from N=46 participants who completed the first session were excluded prior to analysis (see ``Exclusion criteria'' below), leaving a final sample of N=110 participants. These participants were re-invited to complete a follow-up experiment 3 and 14 days later. (There was no follow-up session at 28 days due to the Christmas holiday.) Once invited, participants were permitted 48 hours to complete the follow-up experiment. Participant retention was again high for each follow-up session (Day 3: N=97 [88.2\%]; Day 14: N=99 [90.0\%]). Participants again received a retention bonus of \$1.00 for each completed follow-up session. Detailed demographic information is presented in Table \ref{tab:demographics}. The majority of participants identified as men (65 men; 53 women; 1 non-binary individual; 1 rather not say) and were 39.6 years old on average (range: 23--69 years).

\subsubsection*{Experimental protocol}

The experimental protocol for Experiment 2 was almost identical to Experiment 1. In each session, participants started by completing several self-report questionnaires (see Supplementary Materials for details). Next, participants completed a modified version of the gamified Pavlovian go/no-go task wherein the trial structure had been changed to resemble that of \cite{wittmann2008striatal}. Instead of 8 unique robots each presented for 30 trials, participants now saw a total of 24 unique robots presented for 8, 10, or 12 trials. Each stimulus was presented for fewer trials in order to measure participants' task performance during learning, as opposed to during asymptotic performance, where the expression of Pavlovian biases are typically largest (for similar discussion, see also \cite{zorowitzPLACEHOLDER}). Moreover, robots were presented to participants in mini-batches, each involving four robots and totalling approximately 40 trials. Crucially, within a batch, all four trial types were not necessarily represented (see Figure \ref{fig:figS01} for an example). That is, in any section of the task, participants were not guaranteed to observe one of each type of robot. As such, participants could not rely on a process-of-elimination strategy in order to enhance their performance. Participants completed six mini-batches, which were divided into two blocks of 120 trials each (12 unique robots per block; three robots of each trial type; Figure \ref{fig:task_schematic}D).

The task was unchanged visually except in two respects. First, the scanner colors were now blue and red (instead of blue and orange), and fixed such that they always indicated rewarding and punishing trials, respectively. Second, the symbols on the robots' chestplates were drawn from one of two Brussels Artificial Character Sets \cite{vidal2017bacs} or the English alphabet (randomized within participants across sessions). These new symbols were used in order to accommodate the need for three times the number of distinctly recognizable robots. Pairwise comparisons revealed no significant differences in percent correct responses by character set (all $p > 0.90$, corrected for multiple comparisons). The timing of the task was also unchanged except the response window was shortened (from 1.5 to 1.3 seconds) and the feedback window was lengthened (from 1.0 to 1.2 seconds). 

\subsubsection*{Exclusion criteria}

The data from multiple participants who completed the experiment on Day 0 were excluded prior to analysis for one or both of the following reasons: failing one or more attention checks embedded in the self-report measures (N=30; \cite{zorowitz2023inattentive}); making Go or No-Go responses on more than 90\% of trials (N=5); exhibiting chance-level performance ($<$55\% correct responses) across all trials (N=22). Data from N=46 participants who completed the first session were excluded under these criteria, leaving a final sample of N=110 participants. No exclusions were applied to subsequent session data. 

\subsubsection*{Analyses}

The analyses for Experiment 2 were identical to those for Experiment 1. 

\subsection*{Results}

\subsubsection*{Descriptive analyses}

Trial-by-trial choice behavior for each session is presented in Figure \ref{fig:exp02_behavior}A. In contrast to Experiment 1, performance in each session conformed to the expected pattern of results. The group-averaged performance measures per session are summarized in Figure \ref{fig:exp02_behavior}B. (Complete descriptive statistics are reported in Table~\ref{tab:stats_exp02}.) On Day 0, overall percent correct responses was 67.5\%. Performance improved in all subsequent sessions, but only marginally so. Pairwise comparisons confirmed that performance was significantly greater on Day 3 compared to Day 0 ($p = 0.009$); however, no other comparisons were significant.

%TC:ignore
\begin{figure}[hpt]
    \centerline{\includesvg[inkscapelatex=false,width=1.00\textwidth]{figures/fig04.svg}}
    \caption{Participants in Experiment 2 show smaller or no practice effects on the modified Pavlovian go/no-go task. (A) Group-averaged learning curves for each trial type and session. Shaded regions indicate 95\% bootstrapped confidence intervals. (B) Group-averaged performance for each session. Performance indices from left-to-right: Correct responses, or overall accuracy; Go bias, or difference in accuracy between Go and No-Go trials; Congruency effect, or difference in accuracy between Pavlovian congruent (GW, NGAL) and incongruent (NGW, GAL) trials; and Feedback sensitivity, or the difference in accuracy on trials following veridical and sham feedback. **Denotes significant pairwise difference ($p < 0.05$, corrected for multiple comparisons). (C) The percentage of participants, for each session and trial type, exhibiting at- or below-chance performance ($< 60\%$ response accuracy; grey), intermediate performance ($\geq 60\%$ response accuracy; light blue), or near-perfect performance ($\geq 90\%$ response accuracy; dark blue).}
    \label{fig:exp02_behavior}
\end{figure}
%TC:endignore

In all sessions, participants performed better on Go trials than No-Go trials. The go bias on Day 0 was significantly greater than that for all other sessions (all $p < 0.005$); no other comparisons were significant. Participants also performed better on Pavlovian congruent compared to incongruent trials in all sessions. The Pavlovian bias on Day 0 was significantly greater than that for all other sessions (both $p = 0.027$); no other comparisons were significant. In sum, group-averaged behavior showed evidence of practice effects but these were minor by comparison to those observed in Experiment 1. 

A similar pattern of results was observed for feedback sensitivity. Across sessions, participants made more correct responses following veridical compared to sham feedback. No pairwise comparison was significant (all $p > 0.10$), suggesting that feedback sensitivity was largely conserved across sessions.

Turning next to individual variation in performance, the proportion of participants who exhibited chance-level, intermediate, or near-ceiling performance by session and trial type is presented in Figure \ref{fig:exp02_behavior}C. In contrast to Experiment 1, the majority of participants exhibited intermediate levels of performance across all trial types and sessions (the only exception was for NGW trials on Day 0). Two-way chi-squared tests of independence confirmed that, with an exception for NGW trials, no significant shift in participants' performance across sessions was observed (GW: $\chi^2 (4) = 1.163$, $p = 0.884$; NGW: $\chi^2 (4) = 13.343$, $p = 0.010$; GAL: $\chi^2 (4) = 6.499$, $p = 0.165$; NGAL: $\chi^2 (4) = 5.097$, $p = 0.278$). Thus, the majority of participants exhibited and maintained intermediate levels of performance on the modified Pavlovian go/no-go task.

\subsubsection*{Model comparison}

The results of the model comparison are summarized in Table \ref{tab:exp2_mc_abbr}. Trial-level choice prediction for all models was worse in Experiment 2 than in Experiment 1, which is to be expected insofar that the modified task primarily measures participants' performance during learning (i.e., when choice is most stochastic). As in Experiment 1, collapsing across sessions, the best-fitting model was the most complex one (i.e., independent parameters per outcome domain plus lapse rate; M7). This was also the best-fitting model within each session (Table \ref{tab:exp2_mc_full}). Posterior predictive checks indicated that this model provided excellent fits to the choice data from each session (Figure \ref{fig:exp2_ppc}). 

%TC:ignore
\begin{table}[b!]
    \centering
    \begin{tabular}{clcrr}
        \toprule
        Model & Parameters & Acc. (\%) & \multicolumn{1}{c}{PSIS-LOO} & \multicolumn{1}{c}{$\Delta$ PSIS-LOO (se)} \\
        \midrule
        1 & $\beta, \eta$ & 72.9 & -95806.3 & -6205.2 (73.2) \\
        2 & $\beta, \tau, \eta$ & 76.5 & -99616.0 & -2395.5 (48.9) \\
        3 & $\beta, \tau_+, \tau_-, \eta$ & 77.6 & -101283.0 & -728.5 (28.2) \\
        4 & $\beta_+, \beta_-, \tau_+, \tau_-, \eta$ & 77.5 & -101422.4 & -589.0 (21.1) \\
        5 & $\beta, \tau_+, \tau_-, \eta_+, \eta_-$ & 77.7 & -101519.0 & -492.4 (19.1) \\
        6 & $\beta_+, \beta_- \tau_+, \tau_-, \eta_+, \eta_-$ & 77.8 & -101548.7 & -462.7 (17.2) \\
        7 & $\beta_+, \beta_- \tau_+, \tau_-, \eta_+, \eta_-, \xi$ & 78.1 & -102011.4 & \multicolumn{1}{c}{-} \\
        \bottomrule
\end{tabular}
    \caption{Model comparison collapsing across sessions. Notes: Acc. = trial-level choice prediction accuracy between observed and model-predicted Go responses. PSIS-LOO = approximate leave-one-out cross-validation presented in deviance scale (i.e., smaller numbers indicate better fit). \break $\Delta$ PSIS-LOO = difference in PSIS-LOO values between models.}
    \label{tab:exp2_mc_abbr}
\end{table}
%TC:endignore

\subsubsection*{Model parameters}

The estimated group-level parameters from the best-fitting model are presented in Figure \ref{fig:exp02_modeling}A. Smaller but credible shifts were observed for the reward and punishment sensitivity parameters. Reward sensitivity ($\beta_+$) was credibly larger on Day 14 compared to Days 0 and 3, whereas punishment sensitivity ($\beta_-$) was credibly larger on Days 3 and 14 compared to Day 0. The inverse pattern was observed for the reward ($\eta_+$) and punishment learning rates ($\eta_-$). The approach bias ($\tau_+$) was marginally but credibly larger on Day 0 compared to Days 3 and 14. No credible differences in the avoidance bias ($\tau_-$) across sessions were observed. Therefore, though Pavlovian biases were somewhat diminished, in both absolute and relative (i.e., compared to the outcome sensitivity parameters) terms, they remain largely intact with repeat testing.   

%TC:ignore
\begin{figure}[t!]
    \centerline{\includesvg[inkscapelatex=false,width=1.00\textwidth]{figures/fig05.svg}}
    \caption{Reinforcement learning model parameters in Experiment 2 show improved stability and reliability. (A) Group-level model parameters for each session. Error bars indicate 95\% HDIs. **Denotes pairwise comparison where 95\% HDI of the difference excludes zero. (B) Test-retest reliability estimates for each model parameter. Filled circles denote estimates for Experiment 2; open circles denote estimates from Experiment 1. The grey vertical lines show the change in reliability across experiments. Dotted lines indicates average reliability for Experiment 2. Shaded region indicates conventional range of acceptable reliability ($\rho \geq 0.7$).}
    \label{fig:exp02_modeling}
\end{figure}
%TC:endignore

The estimated test-retest reliability of the model parameters is presented in Figure \ref{fig:exp02_modeling}B. In contrast to Experiment 1, averaging across session-pairs, acceptable test-retest reliability was observed for essentially all parameters (reward sensitivity: $\bar{\rho} = 0.989$, 95\% CI = [0.984, 0.992]; punishment sensitivity: $\bar{\rho} = 0.963$, 95\% CI = [0.949, 0.973]; approach bias: $\bar{\rho} = 0.696$, 95\% CI = [0.622, 0.765]; avoidance bias: $\bar{\rho} = 0.729$, 95\% CI = [0.669, 0.778]; reward learning rate: $\bar{\rho} = 0.861$, 95\% CI = [0.817, 0.898]; punishment learning rate: $\bar{\rho} = 0.768$, 95\% CI = [0.710, 0.815]). Compared to Experiment 1, test-retest reliability was significantly improved for reward sensitivity ($\Delta\bar{\rho} = 0.125$, 95\% CI = [0.101, 0.152]), approach bias ($\Delta\bar{\rho} = 0.318$, 95\% CI = [0.185, 0.446]), avoidance bias ($\Delta\bar{\rho} = 0.227$, $95\% \ \text{CI} = [0.126, 0.315]$), and reward learning rate ($\Delta\bar{\rho} = 0.415$, 95\% CI = [0.294, 0.531]); no parameters showed significantly worsened reliability. A similar pattern of results was observed for the split-half reliability estimates Figure \ref{fig:figS04}B.

\section*{General Discussion}

Despite considerable use in individual-differences research, the Pavlovian go/no-go task has received little psychometric evaluation. The two prior studies of the task both found that descriptive and model-based measures of performance on the task exhibited suboptimal reliability \cite{moutoussis2018change, pike2022test}, but these studies did not utilize a number of strategies previously found to improve task measure reliability. Here, we investigated the psychometric properties of two versions of the Pavlovian go/no-go task. In the first, which was a gamified version of the standard task, we observed considerable practice effects whereby the majority of participants exhibited near-ceiling levels of performance with repeat testing. Consequently, the test-retest reliability of multiple reinforcement learning model parameters, estimated from participants' behavior on the task, was unacceptable. To address these issues, we designed an alternative version of the task that measures choice behavior primarily during learning and prevents undesirable process-of-elimination strategies. Participants exhibited greatly reduced practice effects on the modified version of the task and, as a consequence, the test-retest reliability of reinforcement learning model parameters estimated from participants' choice behavior was significantly improved.

The estimates of model-parameter reliability observed in this study (especially those in Experiment 2) are greater than what has previously been reported \cite{moutoussis2018change, pike2022test}. The discrepancy in findings likely reflects a confluence of factors. First, in contrast to previous research, both versions of the Pavlovian go/no-go task studied here were gamified. Gamification has previously been shown to promote participant engagement and minimize confusion \cite{sailer2017gamification} and benefit the reliability of cognitive task measures \cite{kucina2022solution, verdejo2021unified}. Second, we used a hierarchical Bayesian modeling framework to estimate model parameters for the reliability analyses. Hierarchical models exert a pooling or regularization effect on model parameters, which decreases measurement error and improves estimates of reliability \cite{rouder2019psychometrics, haines2023classical}. Indeed, our results are consistent with previous empirical studies that have demonstrated the benefits of hierarchical Bayesian models for estimating parameter reliability \cite{brown2020improving, waltmann2022sufficient}. Finally, in Experiment 2, we redesigned the trial structure of the Pavlovian go/no-go task in order to diminish practice effects. Practice effects can harm reliability when they induce ceiling effects (as in Experiment 1) or when they are not uniformly expressed by participants (e.g., as a function of age \cite{anokhin2022age}). It is possible that such an effect worsened reliability estimates in one prior study of an adolescent sample where practice effects were observed \cite{moutoussis2018change}. 

The occurrence of practice effects with repeated administrations is common for cognitive tasks \cite{hausknecht2007retesting, scharfen2018retest}. Practice effects may reflect a number of factors, such as reductions in performance anxiety or the acquisition of task-specific knowledge or strategies. In the current study, practice effects were ostensibly attributable to participants adopting a qualitatively different strategy after their initial completion of the Pavlovian go/no-go task. Specifically, participants seemingly used acquired knowledge of implicit dependencies between stimuli in the task, leading them to develop a process-of-elimination strategy. In turn, this allowed participants to rapidly learn the correct response to each stimulus. To address this issue, we redesigned the task to eliminate these dependencies and the formation of such a top-down strategy. This approach is consistent with previous research, wherein preventing participants from becoming aware of critical elements of a task design resulted in improved consistency and reliability of behavior, even with practice \cite{mclean2018towards}. 

It is important to note that, although practice effects were reduced in our modified version of the Pavlovian go/no-go task, they were not eliminated altogether. Indeed, we observed smaller but still significant reductions in participants' go and Pavlovian biases (with corresponding decreases in the approach bias model parameter) following the initial test session. For the purposes of individual-differences correlational research, these residual practice effects are tolerable (i.e., because the reliabilities of the model parameters are still in an acceptable range). However, they may be worrisome for longitudinal studies where systematic changes in task performance of interest (e.g., reduction in Pavlovian biases following psychotherapy \cite{geurts2022aversive}). One possible solution might be increasing the length of the practice block, which was relatively brief in this study, and could be extended to help participants reach ``steady state'' performance prior to starting the actual task.

The current study is not without limitations. Here we have investigated the psychometric properties of two versions of the Pavlovian go/no-go task in a sample of online adult participants. The reliability of task measures, however, are not absolute; reliability can vary as a function of the sample and the test setting. For example, previous research has shown that the reliability of a task completed by healthy adults is not the same as that for adults with psychopathology \cite{cooper2017role} or healthy children \cite{arnon2020current}. Given that the current sample of healthy adult participants rated the modified Pavlovian go/no-go task as more mentally demanding (see Table \ref{tab:appraisals}), it may prove to be too challenging for other groups (e.g., children) which may affect reliability. Future research will be necessary to validate the modified version of the task in other populations. 

A second limitation is that we only studied participants' choice behavior. Previous studies have found that Pavlovian biases also manifest in participants' response times on the task \cite{millner2018pavlovian, algermissen2022striatal}, and these may be a meaningful index of individual differences \cite{betts2020learning, millner2019suicidal, scholz2020dissociable}. Furthermore, previous research has introduced a computational framework for jointly modeling participants' choice and response time behavior on the task \cite{millner2018pavlovian, millner2019suicidal}. This is notable because joint modeling of choice and response time has previously been found to improve the precision and reliability of parameter estimates from reinforcement learning models \cite{ballard2019joint, shahar2019improving}. As such, future research is warranted to investigate how the reliability of model-based measures of behavior on the Pavlovian go/no-go task could be further improved by incorporating response times. 

Limitations notwithstanding, our study demonstrates that it is possible to derive performance measures from the Pavlovian go/no-go task that are sufficiently reliable for use in individual-differences research. We encourage researchers to use and further adapt the modified version of the task presented here. In support of this goal, we have made all of our data and code publicly available (see Data and Code Availability statements).

%TC:ignore
\break
\section*{Data availability}

The data that support the findings of this study are openly available on Github at \url{https://github.com/nivlab/RobotFactory}.

\section*{Code availability}

All code for data cleaning and analysis associated with this study is available at \url{https://github.com/nivlab/RobotFactory}. The experiment code is available at the same link.  The custom web-software for serving online experiments is available at \url{https://github.com/nivlab/nivturk}. A playable demo of the task is available at \url{https://nivlab.github.io/jspsych-demos/tasks/pgng/experiment.html}.

% \section*{Citation diversity statement}

% Recent work in several fields of science has identified a bias in citation practices such that papers from women and other minority scholars are under-cited relative to the number of such papers in the field \cite{dworkin2020extent, bertolero2020racial}. Here we sought to proactively consider choosing references that reflect the diversity of the field in thought, form of contribution, gender, race, ethnicity, and other factors. First, we obtained the predicted gender of the first and last author of each reference by using databases that store the probability of a first name being carried by a woman \cite{dworkin2020extent}. By this measure and excluding self-citations to the first and last authors of our current paper), our references contain 12.0\% woman(first)/woman(last), 16.0\% man/woman, 26.0\% woman/man, and 46.0\% man/man. This method is limited in that a) names, pronouns, and social media profiles used to construct the databases may not, in every case, be indicative of gender identity and b) it cannot account for intersex, non-binary, or transgender people. Second, we obtained predicted racial/ethnic category of the first and last author of each reference by databases that store the probability of a first and last name being carried by an author of color \cite{ambekar2009name, sood2018predicting}. By this measure (and excluding self-citations), our references contain 4.7\% author of color (first)/author of color(last), 14.1\% white author/author of color, 18.2\% author of color/white author, and 63.0\% white author/white author. This method is limited in that a) names and Florida Voter Data to make the predictions may not be indicative of racial/ethnic identity, and b) it cannot account for Indigenous and mixed-race authors, or those who may face differential biases due to the ambiguous racialization or ethnicization of their names.  We look forward to future work that could help us to better understand how to support equitable practices in science.

\section*{Acknowledgements}

The authors are grateful to Daniel Bennett for helpful feedback on the task design. The research reported in this manuscript was supported in part by the National Center for Advancing Translational Sciences (NCATS), a component of the National Institute of Health (NIH), under award number UL1TR003017 (ND, YN). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. SZ was supported by an NSF Graduate Research Fellowship. The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.

\section*{Competing Interests Statement}

The authors declare no competing interests.

\break
\section*{References}
\printbibliography[heading=main]
\end{refsection}

\break
\begin{refsection}[supp]
\section*{Supplementary materials}
\setcounter{figure}{0}
\setcounter{table}{0}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thefigure}{S\arabic{figure}}

\subsection*{Self-report measures}

In Experiment 1, prior to starting the Pavlovian go/no-go task, participants completed the following self-report questionnaires: the 7-item generalized anxiety disorder scale (GAD-7; \cite{spitzer2006brief}); the 14-item manic and depressive tendencies scale (7-up/7-down; \cite{youngstrom20137}); and the abbreviated 12-item behavioral activation/inhibition scale (BIS/BAS; \cite{pagliaccio2016revising}). Participants also indicated their current mood using the affective slider \cite{betella2016affective}. Participants completed the 7-up/7-down and BIS/BAS scales twice, once on Day 0 and Day 28. Participants completed the GAD-7 and mood slider scales at the start of every session. 

In Experiment 2, prior to starting the task, participants completed the same set of self-report questionnaires with the exception that the 7-up/7-down was replaced with the 7-item depression subscale from the depression, anxiety, and stress scale (DASS; \cite{henry2005short}). Participants completed the BIS/BAS scale once, on Day 0 only. Participants completed the GAD-7, DASS, and mood slider scales at the start of every session. 

\subsection*{Reinforcement learning models}

To more precisely characterize participants' performance on the Pavlovian go/no-go task in Experiments 1 and 2, we fit a nested set of reinforcement learning models to their choice data.  All models are variants of the Rescorla-Wagner model and have previously been used to predict choice behavior on this task \cite{guitart2012go, mkrtchian2017modeling, moutoussis2018change, swart2017catecholaminergic}. Under the most complex model (M7), the probability that a participant makes a go response following stimulus $k$ is defined as:
\begin{equation}
    p(y = \text{go}) = (1 - \xi) \cdot \text{logit}^{-1} \left( \beta_{v_k} \cdot [Q_k(\text{Go}) - Q_k(\text{NoGo})] + \tau_{v_k} \right) + \frac{\xi}{2}
\end{equation}
where $Q_k(\text{go})$ and $Q_k(\text{no-go})$ are the learned stimulus-action values for the go and no-go responses, respectively; $\beta_{v_k}$ is the reward sensitivity (if stimulus $k$ is rewarding) or punishment sensitivity (if stimulus $k$ is punishing) parameter; $\tau_{v_k}$ is an approach bias (if stimulus $k$ is rewarding) or avoidance bias (if stimulus $k$ is punishing) parameter; and $\xi$ is the lapse rate. In turn, the value of a response is learned through feedback according to a learning rule:
\begin{equation}
    Q_k(\text{action}) \leftarrow \eta_{v_k} \cdot \left[ r - Q_k(\text{action}) \right]
\end{equation}
where $r$ is the observed outcome on a particular trial; and $\eta_{v_k}$ is the positive learning rate (if stimulus $k$ is rewarding) or the negative learning rate (if stimulus $k$ is punishing). In all models, outcomes were rescaled so that the the better and worse of the two possible outcomes were equal to one and zero, respectively. The Q-values were accordingly initialized to $Q = 0.5$. 

Simplifications of this model involved either fixing parameters to be equal to zero or fixing parameters to be equal their counterpart. Specifically, the base model (M1) had two free parameters: a single outcome sensitivity parameter and a single learning rate, both shared across outcome domains (i.e., $\beta_+ = \beta_-$; $\eta_+ = \eta_-$; $\tau_+ = \tau_- = 0$). Model 2 added a static action bias parameter that was shared across outcome domains (i.e., $\tau_+ = \tau_-$). Model 3 relaxed this assumption and involved independent approach ($\tau_+$) and avoidance ($\tau_-$) parameters. The next two models involved independent outcome sensitivity ($\beta_+, \beta_-$; M4) or learning rate ($\eta_+, \eta_-$; M5) parameters by outcome domain. Model 6 involved both independent outcome sensitivity and learning rate parameters. Finally, the most complex model (M7) included a lapse rate ($\xi$).

All models were estimated within a hierarchical Bayesian modeling framework using Hamiltonian Monte Carlo as implemented in Stan (v2.30; \cite{carpenter2017stan}). For each model, four separate chains with randomized start values each took 7,500 samples from the posterior. The first 5,000 samples, and every-other subsequent sample, from each chain were discarded. Thus, 5,000 post-warmup samples from the joint posterior were retained. The $\hat{R}$ values for all parameters were $\leq 1.01$, indicating acceptable convergence between chains, and there were no divergent transitions in any chain. For all models, we specified diffuse, uninformative priors in order to avoid biasing parameter estimation (Table \ref{tab:priors}).

\subsection*{Reliability analyses}

In order to estimate the reliability of the reinforcement learning model parameters, we used a hierarchical modeling approach in which data were pooled within- and across-participants \cite{rouder2019psychometrics}. Briefly, two sets of reinforcement learning model parameters were estimated per participant and session (in the case of test-retest reliability) or task block (in the case of split-half reliability). Specifically, each parameter ($\theta \in \{\beta, \eta, \tau, \xi \}$) was estimated as follows:
\begin{equation}
\begin{split}
    \theta_{i1} = \mu_1 + \theta_{ic} - \theta_{id} \\
    \theta_{i2} = \mu_2 + \theta_{ic} + \theta_{id}
\end{split}
\end{equation}
where $\theta_{i1}$ and $\theta_{i2}$ are a given parameter (e.g., outcome sensitivity, $\beta$) for participant $i$ in sessions or blocks 1 and 2, respectively; $\mu_1$ and $\mu_2$ are the group-averaged parameters for sessions or blocks 1 and 2; $\theta_{ic}$ is the common effect for participant $i$ (i.e., the component of a parameter that is stable across sessions or blocks); and $\theta_{id}$ is the difference effect for participant $i$ (i.e., the parameter component that is variable across sessions or blocks). The collection of $\theta_{ic}$ parameters constitutes between-participants variability, whereas the collection of $\theta_{id}$ parameters constiutes within-participants variability. Both $\theta_{ic}$ and $\theta_{id}$ were assumed to be normally-distributed with zero means and independent, estimated variances. Split-half and test-retest reliability estimates were calculated by taking the Pearson correlation of $\theta_{i1}$ and $\theta_{i2}$ across task blocks and sessions, respectively.

\clearpage
\subsection*{Comparative stability of self-report measures}

\begin{figure}[h]
    \centerline{\includesvg[inkscapelatex=false,width=1.00\textwidth]{figures/figS05.svg}}
    \caption{Summary of self-report measures indicating that mood and anxiety across participants were relatively stable over the study period. (A) Group-level mood and symptom scores for each session. Error bars indicate 95\% bootstrapped confidence intervals. **Denotes significant pairwise difference ($p < 0.05$, corrected for multiple comparisons). (B) Test-retest reliability estimates for each model parameter. Dotted lines indicates overall average. Shaded region indicates conventional range of acceptable reliability ($\rho \geq 0.7$).}
    \label{fig:figS05}
\end{figure}

\break
\subsection*{Example block of the modified Pavlovian go/no-go task}

\begin{figure}[h]
    \centerline{\includesvg[inkscapelatex=false,width=1.00\textwidth]{figures/figS01.svg}}
    \caption{The trial structure of an example block of the modified Pavlovian go/no-go task. In one task block, there are 12 unique stimuli (three of each trial type: go to win [GW; dark blue]; no-go to win [NGW; light blue]; go to avoid losing [GAL; light red]; no-go to avoid losing [NGAL; dark red]) divided into three sets (Set 1: empty circles; Set 2: half-filled circles; Set 3: filled circles). Each set is composed of approximately 40 trials (120 trials total). Each set, however, may not involve all four trial types. In the example block above, Set 1 involves two NGAL stimuli and zero GAL stimuli, and Set 2 involves two GAL stimuli and zero NGAL stimuli.}
    \label{fig:figS01}
\end{figure}

\break
\subsection*{Posterior predictive check for the best-fitting reinforcement learning model (Experiment 1)}

\begin{figure}[h]
    \centerline{\includesvg[inkscapelatex=false,width=1.00\textwidth]{figures/figS02.svg}}
    \caption{Observed \& model-predicted choice behavior for Experiment 1. (A) Trial-by-trial choice behavior. Solid and dotted lines depict observed and model-predicted choice behavior, respectively. (B) Observed (x-axis) and model-predicted (y-axis) proportions of go responses for each trial type. Each point corresponds to one participant and condition. Notes: RMSE = root-mean-squared error between observed and model-predicted choice behavior. $\rho$ = Spearman's rank correlation between observed and model-predicted choice behavior.}
    \label{fig:exp1_ppc}
\end{figure}

\clearpage
\subsection*{Posterior predictive check for the best-fitting reinforcement learning model (Experiment 2)}

\begin{figure}[h]
    \centerline{\includesvg[inkscapelatex=false,width=1.00\textwidth]{figures/figS03.svg}}
     \caption{Observed \& model-predicted choice behavior for Experiment 2. (A) Trial-by-trial choice behavior. Solid and dotted lines depict observed and model-predicted choice behavior, respectively. (B) Observed (x-axis) and model-predicted (y-axis) proportions of go responses for each trial type. Each point corresponds to one participant and condition. Notes: RMSE = root-mean-squared error between observed and model-predicted choice behavior. $\rho$ = Spearman's rank correlation between observed and model-predicted choice behavior.}
    \label{fig:exp2_ppc}
\end{figure}

\clearpage
\subsection*{Split-half reliability of model parameters}

\begin{figure}[h]
    \centerline{\includesvg[inkscapelatex=false,width=1.00\textwidth]{figures/figS04.svg}}
    \caption{Split-half reliability estimates for the best-fitting model parameters for Experiment 1 (A) and Experiment 2 (B). Filled circles denote estimates for each experiment; open circles denote estimates from Experiment 1. The grey vertical lines show the change in reliability across experiments. Dotted lines indicates average reliability. Shaded region indicates conventional range of acceptable reliability ($\rho \geq 0.7$).}
    \label{fig:figS04}
\end{figure}

\clearpage
\subsection*{Participant demographics}

\begin{table}[h]
    \centering
    \begin{tabular*}{\textwidth}{lccc}
    \toprule
    Variable & \small Experment 1 (N=103) & \small Experiment 2 (N=110)  & p-value \\
    \midrule
    \textbf{Gender}, N (\%)                & & & 0.403 \\
    \hspace{1em} Men                       & 55 (53.4\%) & 65 (59.1\%) & \\
    \hspace{1em} Women                     & 47 (45.6\%) & 43 (39.1\%) & \\
    \hspace{1em} Transgender or nonbinary         &   1  (1.0\%) &   1  (0.9\%) & \\
    \hspace{1em} Rather not say            &   0  (0.0\%) &   1  (0.9\%) & \\
    \midrule
    \textbf{Age}, yrs                      & & & 0.006 \\
    \hspace{1em} Mean (range)              & 35.5 (20--69) & 39.6 (23--69) & \\
    \midrule
    \textbf{Race \& Ethnicity}, N (\%)                  & & & 0.264 \\
    \hspace{1em} White                     & 80 (77.7\%) &  97 (83.6\%) & \\ 
    \hspace{1em} Black or African American &  9  (8.7\%) &  10  (8.6\%) & \\
    \hspace{1em} Hispanic or Latino        &  9  (8.7\%) &  8  (7.3\%) & \\
    \hspace{1em} Asian                     & 10  (9.7\%) &   3  (2.6\%) & \\
    \hspace{1em} American Indian/Alaska Native & 0 (0.0\%) & 3 (2.6\%) & \\
    \hspace{1em} Rather not say            &  4  (3.9\%) &  3  (2.6\%) & \\
    \bottomrule
    \end{tabular*}
    \caption{Demographic characteristics of the participants in Experiments 1 and 2. Notes: Participants were able to select more than one ethnic and racial identity. Therefore, the participant counts and percentages in this section sum to more than 100\%. The mean age of each sample was compared via the independent samples $t$-test ($df = 211$, $\alpha = 0.05$, two-sided). The proportion of participants in each sample identifying as men or white was compared via the two sample proportions $z$-test ($df = 211$, $\alpha = 0.05$, two-sided).}
    \label{tab:demographics}
\end{table}

\clearpage
\subsection*{Task appraisals}

\begin{table}[h!]
    \centering
    \begin{tabular}{lccccc}
    \toprule
               & Study &   Day 0 &      Day 3 &     Day 14 &     Day 28 \\
    \midrule
    Difficulty &  1 &  2.5 (1.1) &  2.2 (0.9) &  2.1 (1.0) &  1.8 (0.8) \\
               &  2 &  3.3 (1.1) &  3.0 (1.1) &  3.1 (1.1) &          - \\
    \midrule
    Fun        &  1 &  3.9 (0.9) &  4.0 (1.0) &  3.9 (0.9) &  4.1 (0.9) \\
               &  2 &  3.9 (0.9) &  4.1 (0.8) &  4.0 (0.8) &          - \\
    \midrule
    Clarity    &  1 &  4.8 (0.6) &  4.9 (0.4) &  5.0 (0.2) &  4.9 (0.3) \\
               &  2 &  4.9 (0.5) &  4.9 (0.2) &  4.9 (0.2) &          - \\
    \bottomrule
    \end{tabular}
    \caption{Mean (sd) of participants' ratings of task difficulty, fun, and clarity. All ratings were made on a 5-point Likert scale. Difficulty: ``How difficult was the task?'' (Very easy = 1, Very hard = 5); Fun: ``How fun was the task?'' (Very boring = 1, Very fun = 5); Clarity: ``How clear were the instructions?'' (Very confusing = 1, Very Clear = 5).}
    \label{tab:appraisals}
\end{table}

\clearpage
\subsection*{Priors for Bayesian reinforcement learning models}

\begin{table}[h!]
    \centering
    \begin{tabular}{llll}
        \toprule
                  &                   & \multicolumn{2}{c}{Group-level} \\
        \cmidrule(lr){3-4}
        Parameter & \multicolumn{1}{c}{Participant-level} & \multicolumn{1}{c}{Mean} & \multicolumn{1}{c}{Std. Dev.} \\
        \midrule
        Outcome sensitivity & $\beta_i \sim 10 \cdot \mathcal{N}(\mu_\beta, \sigma_\beta)$ & $\mu_\beta \sim \mathcal{N}(0,1)$ & $\sigma_\beta \sim \text{Half-}t(3,0,1)$ \\ 
        Approach/avoidance bias & $\tau_i \sim 5 \cdot \mathcal{N}(\mu_\tau, \sigma_\tau)$ & $\mu_\tau \sim \mathcal{N}(0,1)$ & $\sigma_\tau \sim \text{Half-}t(3,0,1)$ \\ 
        Learning rate & $\eta_i \sim \Phi \left(\mathcal{N}(\mu_\eta, \sigma_\eta)\right)$ & $\mu_\eta \sim \mathcal{N}(0,1)$ & $\sigma_\eta \sim \text{Half-}t(3,0,1)$ \\ 
        Lapse rate & $\xi_i \sim \Phi \left(-2 + \mathcal{N}(\mu_\xi, \sigma_\xi)\right)$ & $\mu_\xi \sim \mathcal{N}(0,1)$ & $\sigma_\xi \sim \text{Half-}t(3,0,1)$ \\ 
        \bottomrule
    \end{tabular}
    \caption{Participant- and group-level priors specified for each parameter in the hierarchical Bayesian reinforcement learning models. Notes: $\Phi$ denotes the cumulative density function for the standard normal distribution (used to constrain learning and lapse rates to be in the range $[0,1]$).}
    \label{tab:priors}
\end{table}

\clearpage
\subsection*{Complete descriptive statistics (Experiment 1)}

\begin{table}[h!]
    \centering
    \small
    \begin{adjustbox}{center}
    \begin{tabular}{lcrrrccccc}
        \toprule
                 & \multicolumn{5}{c}{Within-session statistics} & \multicolumn{4}{c}{Between-session comparisons} \\
        \cmidrule(lr){2-6} \cmidrule(lr){7-10}
        Variable & Day & Mdn & \multicolumn{1}{c}{$d$} & \multicolumn{1}{c}{$p$} & 95\% CI & Day 0 & Day 3 & Day 14 & Day 28 \\
        \midrule
         Correct responses (\%)     &  0 & 85.0 &  2.982 & $<$0.001 & [80.8, 87.9] &        - &       &       &   \\
                                    &  3 & 92.9 &  6.947 & $<$0.001 & [90.4, 94.6] & $<$0.001 & -     &       &   \\
                                    & 14 & 94.6 & 10.310 & $<$0.001 & [93.1, 95.2] & $<$0.001 & 0.312 & -     &   \\
                                    & 28 & 94.6 & 12.029 & $<$0.001 & [93.8, 95.8] & $<$0.001 & 0.659 & 0.986 & - \\
        \midrule
        Go bias ($\Delta$\%)        &  0 & 11.7 &  1.049 & $<$0.001 & [10.8, 13.3] &        - &       &       &   \\
                                    &  3 &  5.0 &  1.156 & $<$0.001 & [\ 4.2, \ 5.8] & $<$0.001 & -     &       &   \\
                                    & 14 &  4.2 &  1.124 & $<$0.001 & [\ 2.9, \ 4.6] & $<$0.001 & 0.310 & -     &   \\
                                    & 28 &  3.3 &  0.899 & $<$0.001 & [\ 2.5, \ 4.2] & $<$0.001 & $<$0.001 & 0.515 & - \\
        \midrule
        Valence bias ($\Delta$\%)   &  0 &  0.8 &  0.135 &    0.064 & [\ -0.8, \ 2.5] &        - &       &       &   \\
                                    &  3 &  0.8 &  0.169 &    0.017 & [\ 0.0, \ 2.5]  & 0.992 & -     &       &   \\
                                    & 14 &  0.0 &  0.000 &    0.264 & [\ -0.8, \ 0.8] & 1.000 & 0.807     &       &   \\
                                    & 28 &  0.0 &  0.000 &    0.257 & [\ -0.8, \ 0.8] & 0.851 & 0.851 & 1.000 & - \\       
        \midrule
        Pavlovian bias ($\Delta$\%) &  0 & 9.2 & 1.237 & $<$0.001 & [\ 6.7, 11.7] &        - &       &       &   \\
                                &  3   & 1.7 & 0.450 &  $<$0.001 & [\ 0.8, \ 2.5] & $<$0.001 & -     &       &   \\
                                & 14 & 1.7 & 0.450 &  $<$0.001 & [\ 0.8, \ 2.5] & $<$0.001 & 0.808 & -     &   \\
                                & 28 & 0.8 & 0.225 & 0.001 & [\ 0.0, \ 1.7] & $<$0.001 & 0.270 & 0.998 & - \\
        \midrule
        \footnotesize Feedback  sensitivity ($\Delta$\%) &  0 & 9.4 & 1.250 & $<$0.001 & [\ 7.2, \ 11.5] &        - &       &       &   \\
         &  3 & 4.5 & 0.770 & $<$0.001 & [\ 3.4, \ 6.2] & $<$0.001 & -     &       &   \\
                                          & 14 & 3.0 & 0.581 & $<$0.001 & [\ 1.8, \ 4.7] & $<$0.001 & 0.841 & -     &   \\
                                          & 28 & 2.8 & 0.704 & $<$0.001 & [\ 2.2, \ 4.3] & $<$0.001 & 0.180 & 0.630 & - \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Within- and between-session descriptive statistics for Experiment 1. The between-session values correspond to the p-values for the pairwise comparisons.}
    \label{tab:stats_exp01}
\end{table}

\clearpage
\subsection*{Model comparison by session (Experiment 1)}

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{lcccr}
        \toprule
        Session & Model & Acc. (\%) & PSIS-LOO & \multicolumn{1}{c}{$\Delta$ PSIS-LOO} \\
        \midrule
         Day 0 & 1 & 82.2 & -37241.1 & -2050.3 (42.9) \\
         & 2 & 84.1 & -38327.4 & -964.0 (30.7) \\
         & 3 & 85.2 & -38992.9 & -298.5 (17.7) \\
         & 4 & 85.2 & -39145.6 & -145.8 (11.1) \\
         & 5 & 85.3 & -39129.9 & -161.5 (8.9) \\
         & 6 & 85.4 & -39190.3 & -101.1 (6.7) \\
         & 7 & 85.6 & -39291.4 & \multicolumn{1}{c}{-} \\
         \midrule
         Day 3 & 1 & 88.7 & -38446.8 & -1468.0 (34.0) \\
         & 2 & 90.0 & -38979.3 & -935.5 (27.5) \\
         & 3 & 90.8 & -39492.8 & -422.0 (17.7) \\
         & 4 & 90.8 & -39644.9 & -269.9 (12.9) \\
         & 5 & 90.8 & -39629.8 & -285.0 (12.3) \\
         & 6 & 90.9 & -39666.1 & -248.7 (11.6) \\
         & 7 & 91.0 & -39914.8 & \multicolumn{1}{c}{-} \\
         \midrule
         Day 14 & 1 & 90.1 & -38350.9 & -928.6 (26.5) \\
         & 2 & 91.5 & -38777.0 & -502.5 (19.1) \\
         & 3 & 91.5 & -38971.4 & -308.1 (14.7) \\
         & 4 & 91.7 & -39066.7 & -212.8 (11.6) \\
         & 5 & 91.7 & -39072.2 & -207.3 (11.3) \\
         & 6 & 91.8 & -39093.8 & -185.7 (10.8) \\
         & 7 & 91.9 & -39279.5 & \multicolumn{1}{c}{-} \\
         \midrule
         Day 28 & 1 & 89.4 & -37419.1 & -1155.6 (30.7) \\
         & 2 & 91.0 & -37928.1 & -646.6 (23.5) \\
         & 3 & 92.1 & -38360.7 & -214.1 (11.8) \\
         & 4 & 92.1 & -38404.5 & -170.3 (9.3) \\
         & 5 & 92.0 & -38434.0 & -140.7 (8.5) \\
         & 6 & 92.1 & -38451.6 & -123.1 (7.6) \\
         & 7 & 92.3 & -38574.7 & \multicolumn{1}{c}{-} \\
         \bottomrule
    \end{tabular}
    \caption{Model comparison broken down by session. Notes: Acc. = trial-level choice prediction accuracy between observed and model-predicted Go responses. PSIS-LOO = approximate leave-one-out cross-validation presented in deviance scale (i.e., smaller numbers indicate better fit). \break $\Delta$ PSIS-LOO = difference in LOO values between models.}
    \label{tab:exp1_mc_full}
\end{table}

\clearpage
\subsection*{Complete descriptive statistics (Experiment 2)}

\begin{table}[h!]
    \centering
    \small
    \begin{adjustbox}{center}
    \begin{tabular}{lcrrrcccc}
        \toprule
                 & \multicolumn{5}{c}{Within-session statistics} & \multicolumn{3}{c}{Between-session comparisons} \\
        \cmidrule(lr){2-6} \cmidrule(lr){7-9}
        Variable & Day & Mdn & \multicolumn{1}{c}{$d$} & \multicolumn{1}{c}{$p$} & 95\% CI & Day 0 & Day 3 & Day 14 \\
        \midrule
         Correct responses (\%)     &  0 & 67.5 &  1.828 & $<$0.001 & [65.8, 69.0] & -     &       &   \\
                                    &  3 & 71.7 &  2.063 & $<$0.001 & [69.6, 74.6] & 0.003 & -     &   \\
                                    & 14 & 69.6 &  1.441 & $<$0.001 & [66.5, 73.8] & 0.146 & 0.335 & - \\
        \midrule
        Go bias ($\Delta$\%)        &  0 & 19.2 &  1.108 & $<$0.001 & [15.0, 22.5] & -     &       &   \\
                                    &  3 & 13.3 &  0.899 & $<$0.001 & [10.0, 15.0]  & 0.007 & -     &  \\
                                    & 14 & 14.2 &  0.882 & $<$0.001 & [\ 7.5, 17.5] & 0.003 & 0.382 & -\\
        \midrule
        Valence bias ($\Delta$\%)   &  0 &  -2.5 &  0.289 &    0.002 & [\ -4.2, \ 0.8] &        - &       &   \\
                                    &  3 &  0.8 &  0.096 &    0.245 & [\ -0.8, \ 3.3]  & 0.096 & -     & \\
                                    & 14 &  2.5 &  0.337 &    0.004 & [\ 0.0, \ 4.2] & 0.002 & 0.851     & \\
        \midrule
        Pavlovian bias ($\Delta$\%) &  0 & 12.5 & 1.065 & $<$0.001 & [10.0, 15.0] &        - &       &   \\
                                    &  3 &  8.3 & 0.843 & $<$0.001 & [\ 6.7, 10.0] & 0.027 & -     & \\
                                    & 14 &  7.5 & 0.867 & $<$0.001 & [\ 5.0, \ 9.2] & 0.027 & 0.997 & -    \\
        \midrule
        \footnotesize Feedback sensitivity ($\Delta$\%) &  0 & 28.2 & 2.254 & $<$0.001 & [25.7, 30.9] &        - &       &  \\
                                    &  3 & 29.4 & 2.142 & $<$0.001 & [24.7, 32.0] & 0.632 & -     &      \\
                                    & 14 & 26.8 & 2.267 & $<$0.001 & [24.1, 29.4] & 0.942 & 0.461 & -    \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Within- and between-session descriptive statistics for Experiment 2. The between-session values correspond to the p-values for the pairwise comparisons.}
    \label{tab:stats_exp02}
\end{table}

\clearpage
\subsection*{Model comparison by session (Experiment 2)}

\begin{table}[h]
    \centering
    \begin{tabular}{lcccr}
        \toprule
        Session & Model & Acc. (\%) & PSIS-LOO & \multicolumn{1}{c}{$\Delta$ PSIS-LOO} \\
        \midrule
         Day 0 & 1 & 71.5 & -33517.7 & -2746.3 (48.6) \\
         & 2 & 75.4 & -35145.3 & -1118.8 (33.1) \\
         & 3 & 77.3 & -36002.4 & -261.6 (16.8) \\
         & 4 & 77.2 & -36074.8 & -189.2 (12.0) \\
         & 5 & 77.4 & -36117.1 & -147.0 (10.7) \\
         & 6 & 77.4 & -36124.5 & -139.6 (9.1) \\
         & 7 & 77.6 & -36264.1 & \multicolumn{1}{c}{-} \\
         \midrule
         Day 3 & 1 & 74.0 & -31023.5 & -1656.0 (38.0) \\
         & 2 & 77.4 & -31996.2 & -683.3 (26.0) \\
         & 3 & 78.1 & -32447.1 & -232.5 (15.7) \\
         & 4 & 78.2 & -32513.7 & -165.9 (11.3) \\
         & 5 & 78.1 & -32508.9 & -170.7 (10.6) \\
         & 6 & 78.3 & -32543.9 & -135.7 (9.1) \\
         & 7 & 78.7 & -32679.6 & \multicolumn{1}{c}{-} \\
         \midrule
         Day 14 & 1 & 73.3 & -31265.0 & -1802.8 (39.2) \\
         & 2 & 76.8 & -32474.5 & -593.4 (24.8) \\
         & 3 & 77.4 & -32833.5 & -234.3 (16.4) \\
         & 4 & 77.3 & -32833.9 & -233.9 (13.1) \\
         & 5 & 77.6 & -32893.1 & -174.7 (11.9) \\
         & 6 & 77.6 & -32880.4 & -187.5 (11.3) \\
         & 7 & 78.1 & -33067.8 & \multicolumn{1}{c}{-} \\
         \bottomrule
    \end{tabular}
    \caption{Model comparison broken down by session. Notes: Acc. = trial-level choice prediction accuracy between observed and model-predicted Go responses. LOO = approximate leave-one-out cross-validation presented in deviance scale (i.e., smaller numbers indicate better fit). \break $\Delta$ PSIS-LOO = difference in PSIS-LOO values between models.}
    \label{tab:exp2_mc_full}
\end{table}

\break
\subsection*{Supplementary references}
\printbibliography[heading=supp]
\end{refsection}

%TC:endignore
\end{document}
